{
  "generated_at": "2026-02-10T07:36:05.834Z",
  "counts": {
    "lite": 30,
    "full": 200,
    "total": 230
  },
  "repos": [
    "Lightning-AI/pytorch-lightning",
    "Netflix/metaflow",
    "astropy/astropy",
    "fastapi/fastapi",
    "huggingface/accelerate",
    "huggingface/transformers",
    "huggingface/trl",
    "linkedin/Liger-Kernel",
    "matplotlib/matplotlib",
    "mesonbuild/meson",
    "mlflow/mlflow",
    "mwaskom/seaborn",
    "optuna/optuna",
    "pandas-dev/pandas",
    "pydantic/pydantic",
    "pydata/xarray",
    "pypa/hatch",
    "pypa/packaging",
    "pypa/setuptools",
    "pytest-dev/pytest",
    "python/mypy",
    "scikit-learn/scikit-learn",
    "sphinx-doc/sphinx",
    "sympy/sympy"
  ],
  "items": [
    {
      "id": "full:astropy__astropy.b0db0daa/test_basic_rgb.067e927c.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_basic_rgb.067e927c.lv1",
      "title": "test_basic_rgb.067e927c.lv1",
      "preview": "RGB Image Composition and Scaling Create a system to combine three separate grayscale images (red, green, blue channels) into a single RGB color image with configurable scaling and normalization. The core functionality should: 1. Primary Goal: Transform three aligned grayscale arrays into a properly-scaled RGB image output 2. Key Features: Apply independent intensity mapping and stretching to each color channel Suppo",
      "preview_md": "## Task\n**Task Statement: RGB Image Composition and Scaling**\n\nCreate a system to combine three separate grayscale images (red, green, blue channels) into a single RGB color image with configurable scaling and normalization. The core functionality should:\n\n1. **Primary Goal**: Transform three aligned grayscale arrays into a properly-scaled RGB image output",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_basic_rgb.067e927c.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_basic_rgb.067e927c.lv2",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_basic_rgb.067e927c.lv2",
      "title": "test_basic_rgb.067e927c.lv2",
      "preview": "RGB Image Composition and Scaling Create a system to combine three separate grayscale images (red, green, blue channels) into a single RGB color image with configurable scaling and normalization. The core functionality should: 1. Primary Goal: Transform three aligned grayscale arrays into a properly-scaled RGB image output 2. Key Features: Apply independent intensity mapping and stretching to each color channel Suppo",
      "preview_md": "## Task\n**Task Statement: RGB Image Composition and Scaling**\n\nCreate a system to combine three separate grayscale images (red, green, blue channels) into a single RGB color image with configurable scaling and normalization. The core functionality should:\n\n1. **Primary Goal**: Transform three aligned grayscale arrays into a properly-scaled RGB image output",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_basic_rgb.067e927c.lv2/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_comparison.445d81a3.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_comparison.445d81a3.lv1",
      "title": "test_comparison.445d81a3.lv1",
      "preview": "Cosmology Object Comparison and Parameter Access System Implement a comprehensive comparison and parameter access framework for cosmological models that supports: Core Functionalities: Dynamic parameter access through descriptor-based attributes with validation and unit handling Multi-level equality checking (strict equality vs. physical equivalence) between cosmology instances Format-agnostic comparison allowing con",
      "preview_md": "## Task\n**Task Statement: Cosmology Object Comparison and Parameter Access System**\n\nImplement a comprehensive comparison and parameter access framework for cosmological models that supports:\n\n**Core Functionalities:**\n- Dynamic parameter access through descriptor-based attributes with validation and unit handling",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_comparison.445d81a3.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_compressed.8daeb7d6.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_compressed.8daeb7d6.lv1",
      "title": "test_compressed.8daeb7d6.lv1",
      "preview": "Implement a comprehensive FITS (Flexible Image Transport System) file I/O library that provides functionality for reading, writing, and manipulating astronomical data files. The system should support: 1. Core functionalities: Header parsing and manipulation, image and table data handling, HDU (Header Data Unit) management, and file format validation 2. Main features: Multi-extension FITS file support, data scaling/co",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement a comprehensive FITS (Flexible Image Transport System) file I/O library that provides functionality for reading, writing, and manipulating astronomical data files. The system should support:",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_compressed.8daeb7d6.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_compression_failures.48dc420b.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_compression_failures.48dc420b.lv1",
      "title": "test_compression_failures.48dc420b.lv1",
      "preview": "FITS File Processing and Header Management Task Implement a system for parsing, manipulating, and processing FITS (Flexible Image Transport System) astronomical data files with focus on: Core Functionalities: Parse and validate FITS table column formats (ASCII and binary) Convert between different data type representations and format specifications Manage FITS headers with keyword-value-comment card structures Handle",
      "preview_md": "## Task\n**FITS File Processing and Header Management Task**\n\nImplement a system for parsing, manipulating, and processing FITS (Flexible Image Transport System) astronomical data files with focus on:\n\n**Core Functionalities:**\n- Parse and validate FITS table column formats (ASCII and binary)",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_compression_failures.48dc420b.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_containers.6079987d.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_containers.6079987d.lv1",
      "title": "test_containers.6079987d.lv1",
      "preview": "Implement Uncertainty Distribution System Create a flexible uncertainty distribution framework that handles arrays and scalars with associated probability distributions. The system should: Core Functionalities: Dynamically generate distribution classes for different array types (numpy arrays, quantities, etc.) Store and manipulate uncertainty samples along trailing array dimensions Provide statistical operations (mea",
      "preview_md": "## Task\n**Task Statement: Implement Uncertainty Distribution System**\n\nCreate a flexible uncertainty distribution framework that handles arrays and scalars with associated probability distributions. The system should:\n\n**Core Functionalities:**\n- Dynamically generate distribution classes for different array types (numpy arrays, quantities, etc.)",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_containers.6079987d.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_converter.2ef0539f.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_converter.2ef0539f.lv1",
      "title": "test_converter.2ef0539f.lv1",
      "preview": "VOTable Data Processing and Validation Task Develop a system to parse, validate, and manipulate VOTable (Virtual Observatory Table) XML files with the following core functionalities: 1. XML Parsing and Tree Construction: Parse VOTable XML documents into structured tree representations with proper element hierarchy (VOTABLE, RESOURCE, TABLE, FIELD, PARAM, etc.) 2. Data Type Conversion: Convert between various astronom",
      "preview_md": "## Task\n**VOTable Data Processing and Validation Task**\n\nDevelop a system to parse, validate, and manipulate VOTable (Virtual Observatory Table) XML files with the following core functionalities:",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_converter.2ef0539f.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_functions.40530bf7.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_functions.40530bf7.lv1",
      "title": "test_functions.40530bf7.lv1",
      "preview": "Implement Unit-Aware Array Operations and Masked Data Handling Core Functionalities: Provide helper functions for numpy array operations that preserve and validate physical units Support masked array operations with proper mask propagation and data handling Enable seamless integration between unit-aware computations and masked data structures Main Features & Requirements: Unit conversion and validation for array conc",
      "preview_md": "## Task\n**Task Statement: Implement Unit-Aware Array Operations and Masked Data Handling**\n\n**Core Functionalities:**\n- Provide helper functions for numpy array operations that preserve and validate physical units\n- Support masked array operations with proper mask propagation and data handling",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_functions.40530bf7.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_lombscargle_multiband.78687278.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_lombscargle_multiband.78687278.lv1",
      "title": "test_lombscargle_multiband.78687278.lv1",
      "preview": "Implement a comprehensive unit and array handling system for scientific computing that provides: 1. Core functionalities: Unit conversion and validation for physical quantities Array broadcasting and masking operations for heterogeneous data Time/date manipulation with high precision arithmetic Table data structure with mixed column types 2. Main features and requirements: Support arbitrary unit assignments and conve",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement a comprehensive unit and array handling system for scientific computing that provides:\n\n1. **Core functionalities:**\n   - Unit conversion and validation for physical quantities\n   - Array broadcasting and masking operations for heterogeneous data",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_lombscargle_multiband.78687278.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_quantity_erfa_ufuncs.3fc54395.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_quantity_erfa_ufuncs.3fc54395.lv1",
      "title": "test_quantity_erfa_ufuncs.3fc54395.lv1",
      "preview": "Implement Structured Unit Operations and Quantity Function Helpers Core Functionalities: Implement arithmetic and indexing operations for structured units (units with multiple named fields like position/velocity pairs) Provide unit-aware helper functions for NumPy array operations that preserve dimensional analysis Support mathematical operations (einsum, norm, concatenation) while maintaining proper unit propagation",
      "preview_md": "## Task\n**Task Statement: Implement Structured Unit Operations and Quantity Function Helpers**\n\n**Core Functionalities:**\n- Implement arithmetic and indexing operations for structured units (units with multiple named fields like position/velocity pairs)\n- Provide unit-aware helper functions for NumPy array operations that preserve dimensional analysis",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_quantity_erfa_ufuncs.3fc54395.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_realizations.5b9cf523.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_realizations.5b9cf523.lv1",
      "title": "test_realizations.5b9cf523.lv1",
      "preview": "Implement context management and serialization functionality for scientific computing objects, focusing on: 1. Core Functionalities: Context management for unit registries and cosmology configurations Object state serialization/deserialization for scientific data structures Data validation and type conversion for column information systems Dynamic attribute access and module-level object discovery 2. Main Features & ",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement context management and serialization functionality for scientific computing objects, focusing on:\n\n1. **Core Functionalities:**\n   - Context management for unit registries and cosmology configurations\n   - Object state serialization/deserialization for scientific data structures",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_realizations.5b9cf523.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_table.48eef659.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_table.48eef659.lv1",
      "title": "test_table.48eef659.lv1",
      "preview": "Implement VOTable Data Processing and Table Operations Core Functionalities: Process and manipulate astronomical data tables in VOTable format with support for various data types (numeric, string, complex, boolean, arrays) Handle table creation, modification, and serialization operations including column addition, row manipulation, and format conversion Provide data validation and format compliance checking for VOTab",
      "preview_md": "## Task\n**Task Statement: Implement VOTable Data Processing and Table Operations**\n\n**Core Functionalities:**\n- Process and manipulate astronomical data tables in VOTable format with support for various data types (numeric, string, complex, boolean, arrays)",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_table.48eef659.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_tilde_path.383d7c0c.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_tilde_path.383d7c0c.lv1",
      "title": "test_tilde_path.383d7c0c.lv1",
      "preview": "FITS File Header and Data Management Develop a system for reading, writing, and manipulating FITS (Flexible Image Transport System) file headers and data. The system should provide: Core Functionalities: Read/write FITS file headers and extract specific keyword values Access and modify header cards with support for commentary keywords (HISTORY, COMMENT) Retrieve image/table data from FITS files with optional scaling ",
      "preview_md": "## Task\n**Task Statement: FITS File Header and Data Management**\n\nDevelop a system for reading, writing, and manipulating FITS (Flexible Image Transport System) file headers and data. The system should provide:\n\n**Core Functionalities:**\n- Read/write FITS file headers and extract specific keyword values",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_tilde_path.383d7c0c.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_tiled_compression.a4fe298a.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_tiled_compression.a4fe298a.lv1",
      "title": "test_tiled_compression.a4fe298a.lv1",
      "preview": "FITS File I/O and Data Processing System Develop a comprehensive system for reading, writing, and manipulating FITS (Flexible Image Transport System) astronomical data files. The system should provide: Core Functionalities: Parse and validate FITS headers with keyword-value pairs and comments Handle both image data (arrays) and table data (binary/ASCII) formats Support file I/O operations with various compression for",
      "preview_md": "## Task\n**Task Statement: FITS File I/O and Data Processing System**\n\nDevelop a comprehensive system for reading, writing, and manipulating FITS (Flexible Image Transport System) astronomical data files. The system should provide:\n\n**Core Functionalities:**\n- Parse and validate FITS headers with keyword-value pairs and comments",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_tiled_compression.a4fe298a.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_tree.261bac3c.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_tree.261bac3c.lv1",
      "title": "test_tree.261bac3c.lv1",
      "preview": "VOTable XML Processing and Validation Task Implement a comprehensive VOTable (Virtual Observatory Table) parser and generator that can: Core Functionalities: Parse VOTable XML files into structured Python objects with full schema validation Generate valid VOTable XML output from Python data structures Handle multiple VOTable format versions (1.1-1.5) with version-specific compliance checking Support various data seri",
      "preview_md": "## Task\n**VOTable XML Processing and Validation Task**\n\nImplement a comprehensive VOTable (Virtual Observatory Table) parser and generator that can:\n\n**Core Functionalities:**\n- Parse VOTable XML files into structured Python objects with full schema validation\n- Generate valid VOTable XML output from Python data structures",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_tree.261bac3c.lv1/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_tree.261bac3c.lv2",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_tree.261bac3c.lv2",
      "title": "test_tree.261bac3c.lv2",
      "preview": "VOTable XML Processing and Validation Task Implement a comprehensive VOTable (Virtual Observatory Table) parser and generator that can: Core Functionalities: Parse VOTable XML files into structured Python objects with full schema validation Generate valid VOTable XML output from Python data structures Handle multiple VOTable format versions (1.1-1.5) with version-specific compliance checking Support various data seri",
      "preview_md": "## Task\n**VOTable XML Processing and Validation Task**\n\nImplement a comprehensive VOTable (Virtual Observatory Table) parser and generator that can:\n\n**Core Functionalities:**\n- Parse VOTable XML files into structured Python objects with full schema validation\n- Generate valid VOTable XML output from Python data structures",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_tree.261bac3c.lv2/problem_statement.md"
    },
    {
      "id": "full:astropy__astropy.b0db0daa/test_vo.8fd473ce.lv1",
      "split": "full",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_vo.8fd473ce.lv1",
      "title": "test_vo.8fd473ce.lv1",
      "preview": "VOTable XML Processing and Validation Task Implement a comprehensive VOTable (Virtual Observatory Table) processing system that can: 1. Parse and validate VOTable XML files - Read VOTable documents from various sources (files, URLs, streams) with configurable validation levels and error handling strategies 2. Extract and manipulate table data - Parse different data serialization formats (TABLEDATA, BINARY, BINARY2, F",
      "preview_md": "## Task\n**VOTable XML Processing and Validation Task**\n\nImplement a comprehensive VOTable (Virtual Observatory Table) processing system that can:\n\n1. **Parse and validate VOTable XML files** - Read VOTable documents from various sources (files, URLs, streams) with configurable validation levels and error handling strategies",
      "statement_path": "./data/full/astropy__astropy.b0db0daa/test_vo.8fd473ce.lv1/problem_statement.md"
    },
    {
      "id": "full:fastapi__fastapi.02e108d1/test_compat.71e8518f.lv1",
      "split": "full",
      "repo_ref": "fastapi__fastapi.02e108d1",
      "repo": "fastapi/fastapi",
      "case": "test_compat.71e8518f.lv1",
      "title": "test_compat.71e8518f.lv1",
      "preview": "Cross-Version Pydantic Compatibility Layer Develop a compatibility abstraction layer that provides unified interfaces for working with Pydantic models and fields across different versions (v1 and v2). The system should: Core Functionalities: Detect and handle both Pydantic v1 and v2 model structures seamlessly Extract and process model field information regardless of underlying Pydantic version Analyze type annotatio",
      "preview_md": "## Task\n**Task Statement: Cross-Version Pydantic Compatibility Layer**\n\nDevelop a compatibility abstraction layer that provides unified interfaces for working with Pydantic models and fields across different versions (v1 and v2). The system should:\n\n**Core Functionalities:**\n- Detect and handle both Pydantic v1 and v2 model structures seamlessly",
      "statement_path": "./data/full/fastapi__fastapi.02e108d1/test_compat.71e8518f.lv1/problem_statement.md"
    },
    {
      "id": "full:fastapi__fastapi.02e108d1/test_compat.71e8518f.lv2",
      "split": "full",
      "repo_ref": "fastapi__fastapi.02e108d1",
      "repo": "fastapi/fastapi",
      "case": "test_compat.71e8518f.lv2",
      "title": "test_compat.71e8518f.lv2",
      "preview": "Cross-Version Pydantic Compatibility Layer Develop a compatibility abstraction layer that provides unified interfaces for working with Pydantic models and fields across different versions (v1 and v2). The system should: Core Functionalities: Detect and handle both Pydantic v1 and v2 model structures seamlessly Extract and process model field information regardless of underlying Pydantic version Analyze type annotatio",
      "preview_md": "## Task\n**Task Statement: Cross-Version Pydantic Compatibility Layer**\n\nDevelop a compatibility abstraction layer that provides unified interfaces for working with Pydantic models and fields across different versions (v1 and v2). The system should:\n\n**Core Functionalities:**\n- Detect and handle both Pydantic v1 and v2 model structures seamlessly",
      "statement_path": "./data/full/fastapi__fastapi.02e108d1/test_compat.71e8518f.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__accelerate.a73fd3a1/test_big_modeling.a10a845e.lv1",
      "split": "full",
      "repo_ref": "huggingface__accelerate.a73fd3a1",
      "repo": "huggingface/accelerate",
      "case": "test_big_modeling.a10a845e.lv1",
      "title": "test_big_modeling.a10a845e.lv1",
      "preview": "Large Model Memory Management and Device Dispatch System Implement a comprehensive system for managing large neural network models across multiple devices (GPU, CPU, disk) with intelligent memory optimization. The system should provide: Core Functionalities: Automatic device mapping and model dispatch across heterogeneous hardware Dynamic parameter offloading to CPU/disk with on-demand loading Memory-efficient model ",
      "preview_md": "## Task\n**Task Statement: Large Model Memory Management and Device Dispatch System**\n\nImplement a comprehensive system for managing large neural network models across multiple devices (GPU, CPU, disk) with intelligent memory optimization. The system should provide:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/huggingface__accelerate.a73fd3a1/test_big_modeling.a10a845e.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_candidate_generator.bc0ea399.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_candidate_generator.bc0ea399.lv1",
      "title": "test_candidate_generator.bc0ea399.lv1",
      "preview": "Implement interfaces for speculative decoding and assisted text generation systems that optimize language model inference through candidate token prediction and validation. Core Functionalities: Generate candidate token sequences using smaller assistant models or lookup strategies Translate tokens and logits between different model vocabularies Cache vocabulary translators for cross-tokenizer compatibility Manage mod",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement interfaces for **speculative decoding and assisted text generation** systems that optimize language model inference through candidate token prediction and validation.\n\n**Core Functionalities:**\n- Generate candidate token sequences using smaller assistant models or lookup strategies",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_candidate_generator.bc0ea399.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_chat_template_utils.8c88f8ec.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_chat_template_utils.8c88f8ec.lv1",
      "title": "test_chat_template_utils.8c88f8ec.lv1",
      "preview": "JSON Schema Generation for Function Tool Integration Generate JSON schemas from Python functions to enable their use as tools in chat templates and conversational AI systems. The core functionality involves: 1. Schema Generation: Convert Python functions with type hints and Google-style docstrings into standardized JSON schemas containing function metadata, parameter specifications, and validation rules. 2. Type Syst",
      "preview_md": "## Task\n**Task Statement: JSON Schema Generation for Function Tool Integration**\n\nGenerate JSON schemas from Python functions to enable their use as tools in chat templates and conversational AI systems. The core functionality involves:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_chat_template_utils.8c88f8ec.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_feature_extraction_auto.13dd67ea.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_feature_extraction_auto.13dd67ea.lv1",
      "title": "test_feature_extraction_auto.13dd67ea.lv1",
      "preview": "Implement Auto-Registration and Factory Pattern for Transformer Model Components Core Functionalities: Implement automatic model configuration and feature extractor discovery and instantiation Provide factory methods for creating appropriate model components based on model types or pretrained paths Enable dynamic registration of custom model configurations and feature extractors Main Features & Requirements: Auto-det",
      "preview_md": "## Task\n**Task Statement: Implement Auto-Registration and Factory Pattern for Transformer Model Components**\n\n**Core Functionalities:**\n- Implement automatic model configuration and feature extractor discovery and instantiation\n- Provide factory methods for creating appropriate model components based on model types or pretrained paths",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_feature_extraction_auto.13dd67ea.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_colpali.370cfbdd.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_colpali.370cfbdd.lv1",
      "title": "test_modeling_colpali.370cfbdd.lv1",
      "preview": "Vision-Language Model Configuration for Document Retrieval Create a configuration system for ColPali models that combines vision and language capabilities for efficient document retrieval. The system should: 1. Core Functionality: Configure multi-modal models that process both visual and textual information to generate embeddings for document retrieval tasks 2. Key Features: Support flexible VLM backbone selection (d",
      "preview_md": "## Task\n**Task Statement: Vision-Language Model Configuration for Document Retrieval**\n\nCreate a configuration system for ColPali models that combines vision and language capabilities for efficient document retrieval. The system should:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_colpali.370cfbdd.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_informer.de781186.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_informer.de781186.lv1",
      "title": "test_modeling_informer.de781186.lv1",
      "preview": "Implement Informer Time Series Model Configuration and Positional Embedding Components Core Functionalities: Configure and initialize the Informer transformer model for time series forecasting with customizable architecture parameters, feature handling, and attention mechanisms Calculate dynamic feature dimensions based on input characteristics and embedding configurations Generate sinusoidal positional embeddings fo",
      "preview_md": "## Task\n**Task Statement: Implement Informer Time Series Model Configuration and Positional Embedding Components**\n\n**Core Functionalities:**\n- Configure and initialize the Informer transformer model for time series forecasting with customizable architecture parameters, feature handling, and attention mechanisms",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_informer.de781186.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_lightglue.3d3d57cb.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_lightglue.3d3d57cb.lv1",
      "title": "test_modeling_lightglue.3d3d57cb.lv1",
      "preview": "Configure a LightGlue keypoint matching model by setting up architecture parameters, attention mechanisms, and confidence thresholds. The task requires handling multi-head attention configurations, integrating keypoint detector settings, defining layer dimensions, and managing early stopping criteria for efficient feature matching between images. Key Requirements: Configure attention layers (heads, dimensions, dropou",
      "preview_md": "## Task\n**Task Statement:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_lightglue.3d3d57cb.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_mimi.3118abde.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_mimi.3118abde.lv1",
      "title": "test_modeling_mimi.3118abde.lv1",
      "preview": "Audio Model Configuration Management Implement a configuration system for the Mimi audio processing model that handles: 1. Core Functionality: Initialize and manage audio model parameters including sampling rates, quantization settings, transformer architecture, and convolution layers for audio encoding/decoding 2. Main Features: Configure audio processing parameters (sampling rate, channels, frame rates) Set up vect",
      "preview_md": "## Task\n**Task Statement: Audio Model Configuration Management**\n\nImplement a configuration system for the Mimi audio processing model that handles:\n\n1. **Core Functionality**: Initialize and manage audio model parameters including sampling rates, quantization settings, transformer architecture, and convolution layers for audio encoding/decoding",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_mimi.3118abde.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_pixtral.a620bb0b.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_pixtral.a620bb0b.lv1",
      "title": "test_modeling_pixtral.a620bb0b.lv1",
      "preview": "Configure a vision transformer model for the Pixtral architecture by implementing parameter initialization and validation. The task involves setting up model dimensions (hidden size, layers, attention heads), image processing parameters (patch size, image dimensions), and specialized components like RoPE (Rotary Position Embeddings). Key considerations include parameter validation, backward compatibility with differe",
      "preview_md": "## Task\n**Task Statement:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_pixtral.a620bb0b.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_pixtral.a620bb0b.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_pixtral.a620bb0b.lv2",
      "title": "test_modeling_pixtral.a620bb0b.lv2",
      "preview": "Configure a vision transformer model for the Pixtral architecture by implementing parameter initialization and validation. The task involves setting up model dimensions (hidden size, layers, attention heads), image processing parameters (patch size, image dimensions), and specialized components like RoPE (Rotary Position Embeddings). Key considerations include parameter validation, backward compatibility with differe",
      "preview_md": "## Task\n**Task Statement:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_pixtral.a620bb0b.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_prompt_depth_anything.2d54afdc.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_prompt_depth_anything.2d54afdc.lv1",
      "title": "test_modeling_prompt_depth_anything.2d54afdc.lv1",
      "preview": "Model Configuration Management and Initialization Implement functionality for managing and initializing machine learning model configurations with the following core requirements: 1. Configuration Comparison and Serialization: Create utilities to compute differences between nested configuration dictionaries, enabling efficient storage by only preserving non-default values while handling recursive nested structures. 2",
      "preview_md": "## Task\n**Task Statement: Model Configuration Management and Initialization**\n\nImplement functionality for managing and initializing machine learning model configurations with the following core requirements:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_prompt_depth_anything.2d54afdc.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_seggpt.f9e83f60.lv1",
      "title": "test_modeling_seggpt.f9e83f60.lv1",
      "preview": "Model Interface Implementation and Configuration Implement code generation for transformer model interfaces focusing on: 1. Core Functionalities: Input embedding retrieval from neural network modules Model configuration initialization with parameter validation and setup 2. Main Features & Requirements: Handle various embedding layer architectures (standard, encoder-decoder, vision-language models) Support flexible co",
      "preview_md": "## Task\n**Task Statement: Model Interface Implementation and Configuration**\n\nImplement code generation for transformer model interfaces focusing on:\n\n1. **Core Functionalities**:\n   - Input embedding retrieval from neural network modules\n   - Model configuration initialization with parameter validation and setup",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_seggpt.f9e83f60.lv2",
      "title": "test_modeling_seggpt.f9e83f60.lv2",
      "preview": "Model Interface Implementation and Configuration Implement code generation for transformer model interfaces focusing on: 1. Core Functionalities: Input embedding retrieval from neural network modules Model configuration initialization with parameter validation and setup 2. Main Features & Requirements: Handle various embedding layer architectures (standard, encoder-decoder, vision-language models) Support flexible co",
      "preview_md": "## Task\n**Task Statement: Model Interface Implementation and Configuration**\n\nImplement code generation for transformer model interfaces focusing on:\n\n1. **Core Functionalities**:\n   - Input embedding retrieval from neural network modules\n   - Model configuration initialization with parameter validation and setup",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_speech_encoder_decoder.b0d78d76.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_speech_encoder_decoder.b0d78d76.lv1",
      "title": "test_modeling_speech_encoder_decoder.b0d78d76.lv1",
      "preview": "Speech-to-Text Model Configuration and Positional Embedding Implementation Implement configuration initialization and positional embedding generation for speech-to-text transformer models. The task involves: Core Functionalities: Configure Speech2Text and Wav2Vec2 model architectures with encoder-decoder parameters, attention settings, and convolutional feature extraction Generate sinusoidal positional embeddings for",
      "preview_md": "## Task\n**Task Statement: Speech-to-Text Model Configuration and Positional Embedding Implementation**\n\nImplement configuration initialization and positional embedding generation for speech-to-text transformer models. The task involves:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_speech_encoder_decoder.b0d78d76.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_speech_to_text.e53c8eb4.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_speech_to_text.e53c8eb4.lv1",
      "title": "test_modeling_speech_to_text.e53c8eb4.lv1",
      "preview": "Speech-to-Text Model Configuration and Audio Processing Implement a speech-to-text transformer model system that: 1. Core Functionalities: Configure encoder-decoder architecture parameters for speech recognition Process audio feature sequences through convolutional subsampling Handle variable-length audio inputs with proper attention masking 2. Main Features & Requirements: Initialize model configuration with audio-s",
      "preview_md": "## Task\n**Task Statement: Speech-to-Text Model Configuration and Audio Processing**\n\nImplement a speech-to-text transformer model system that:\n\n1. **Core Functionalities:**\n   - Configure encoder-decoder architecture parameters for speech recognition\n   - Process audio feature sequences through convolutional subsampling",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_speech_to_text.e53c8eb4.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_speech_to_text.e53c8eb4.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_speech_to_text.e53c8eb4.lv2",
      "title": "test_modeling_speech_to_text.e53c8eb4.lv2",
      "preview": "Speech-to-Text Model Configuration and Audio Processing Implement a speech-to-text transformer model system that: 1. Core Functionalities: Configure encoder-decoder architecture parameters for speech recognition Process audio feature sequences through convolutional subsampling Handle variable-length audio inputs with proper attention masking 2. Main Features & Requirements: Initialize model configuration with audio-s",
      "preview_md": "## Task\n**Task Statement: Speech-to-Text Model Configuration and Audio Processing**\n\nImplement a speech-to-text transformer model system that:\n\n1. **Core Functionalities:**\n   - Configure encoder-decoder architecture parameters for speech recognition\n   - Process audio feature sequences through convolutional subsampling",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_speech_to_text.e53c8eb4.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_swin2sr.f5cba486.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_swin2sr.f5cba486.lv1",
      "title": "test_modeling_swin2sr.f5cba486.lv1",
      "preview": "Swin2SR Model Configuration Setup Configure a Swin Transformer v2-based super-resolution model by implementing parameter initialization for image enhancement tasks. The system should handle architectural specifications including transformer layers, attention mechanisms, and upscaling components while supporting multiple image processing modes (super-resolution, denoising, artifact reduction). Key considerations inclu",
      "preview_md": "## Task\n**Task Statement: Swin2SR Model Configuration Setup**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_swin2sr.f5cba486.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_swin2sr.f5cba486.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_swin2sr.f5cba486.lv2",
      "title": "test_modeling_swin2sr.f5cba486.lv2",
      "preview": "Swin2SR Model Configuration Setup Configure a Swin Transformer v2-based super-resolution model by implementing parameter initialization for image enhancement tasks. The system should handle architectural specifications including transformer layers, attention mechanisms, and upscaling components while supporting multiple image processing modes (super-resolution, denoising, artifact reduction). Key considerations inclu",
      "preview_md": "## Task\n**Task Statement: Swin2SR Model Configuration Setup**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_swin2sr.f5cba486.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_time_series_transformer.b0ef2ff5.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_time_series_transformer.b0ef2ff5.lv1",
      "title": "test_modeling_time_series_transformer.b0ef2ff5.lv1",
      "preview": "Configure a Time Series Transformer model by implementing initialization and feature calculation methods. The task requires setting up model architecture parameters (encoder/decoder layers, attention heads, dimensions), time series-specific settings (prediction length, context length, lag sequences), and feature handling (static/dynamic features, embeddings). Key challenges include validating parameter consistency be",
      "preview_md": "## Task\n**Task Statement:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_time_series_transformer.b0ef2ff5.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_vitmatte.2d342386.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_vitmatte.2d342386.lv1",
      "title": "test_modeling_vitmatte.2d342386.lv1",
      "preview": "Configuration Management and Comparison for Vision Transformer Models Core Functionalities: Initialize and configure vision transformer model architectures (VitMatte, VitDet, DINOv2) with customizable parameters Compare and compute differences between nested configuration dictionaries recursively Handle model-specific parameters like backbone configurations, attention mechanisms, and architectural variants Main Featu",
      "preview_md": "## Task\n**Task Statement: Configuration Management and Comparison for Vision Transformer Models**\n\n**Core Functionalities:**\n- Initialize and configure vision transformer model architectures (VitMatte, VitDet, DINOv2) with customizable parameters\n- Compare and compute differences between nested configuration dictionaries recursively",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_vitmatte.2d342386.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_vitpose.30ffd86e.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_vitpose.30ffd86e.lv1",
      "title": "test_modeling_vitpose.30ffd86e.lv1",
      "preview": "VitPose Model Configuration Implementation Implement configuration classes for a Vision Transformer-based pose estimation model that supports: 1. Core Functionalities: Configure ViT backbone architecture with transformer layers, attention mechanisms, and patch embedding parameters Set up pose estimation decoder with upscaling and heatmap generation options Support modular backbone integration with flexible output fea",
      "preview_md": "## Task\n**Task Statement: VitPose Model Configuration Implementation**\n\nImplement configuration classes for a Vision Transformer-based pose estimation model that supports:\n\n1. **Core Functionalities:**\n   - Configure ViT backbone architecture with transformer layers, attention mechanisms, and patch embedding parameters",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_vitpose.30ffd86e.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_vitpose.30ffd86e.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_vitpose.30ffd86e.lv2",
      "title": "test_modeling_vitpose.30ffd86e.lv2",
      "preview": "VitPose Model Configuration Implementation Implement configuration classes for a Vision Transformer-based pose estimation model that supports: 1. Core Functionalities: Configure ViT backbone architecture with transformer layers, attention mechanisms, and patch embedding parameters Set up pose estimation decoder with upscaling and heatmap generation options Support modular backbone integration with flexible output fea",
      "preview_md": "## Task\n**Task Statement: VitPose Model Configuration Implementation**\n\nImplement configuration classes for a Vision Transformer-based pose estimation model that supports:\n\n1. **Core Functionalities:**\n   - Configure ViT backbone architecture with transformer layers, attention mechanisms, and patch embedding parameters",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_vitpose.30ffd86e.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_vits.6379a2ba.lv2",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_vits.6379a2ba.lv2",
      "title": "test_modeling_vits.6379a2ba.lv2",
      "preview": "VITS Text-to-Speech Model Configuration Configure a VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model by implementing a comprehensive configuration class that manages all architectural and training parameters. Core Functionalities: Initialize and validate configuration parameters for a complete text-to-speech pipeline Support both single-speaker and multi-speaker TTS scenarios",
      "preview_md": "## Task\n**Task Statement: VITS Text-to-Speech Model Configuration**\n\nConfigure a VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model by implementing a comprehensive configuration class that manages all architectural and training parameters.\n\n**Core Functionalities:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_vits.6379a2ba.lv2/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_modeling_zoedepth.e19636d3.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_zoedepth.e19636d3.lv1",
      "title": "test_modeling_zoedepth.e19636d3.lv1",
      "preview": "Model Configuration Management and Initialization Implement functionality for managing machine learning model configurations with the following core requirements: 1. Configuration Initialization: Create standardized initialization methods for various vision transformer models (DINOv2, ZoeDepth, BEiT) that handle model-specific parameters like hidden dimensions, attention heads, layer counts, and architectural setting",
      "preview_md": "## Task\n**Task Statement: Model Configuration Management and Initialization**\n\nImplement functionality for managing machine learning model configurations with the following core requirements:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_modeling_zoedepth.e19636d3.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processing_git.7342da79.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_git.7342da79.lv1",
      "title": "test_processing_git.7342da79.lv1",
      "preview": "Image Preprocessing Pipeline for Vision Models Implement a comprehensive image preprocessing system that handles RGB conversion, resizing, and batch processing for computer vision models. The core functionalities include: 1. RGB Format Conversion: Convert images from various color formats (e.g., grayscale, RGBA) to standard RGB format for model compatibility 2. Intelligent Image Resizing: Resize images while preservi",
      "preview_md": "## Task\n**Task Statement: Image Preprocessing Pipeline for Vision Models**\n\nImplement a comprehensive image preprocessing system that handles RGB conversion, resizing, and batch processing for computer vision models. The core functionalities include:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processing_git.7342da79.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processing_grounding_dino.3d24a7e6.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_grounding_dino.3d24a7e6.lv1",
      "title": "test_processing_grounding_dino.3d24a7e6.lv1",
      "preview": "Transformers Auto-Factory and Component Loading System Develop a dynamic model and tokenizer auto-loading system that: 1. Core Functionality: Automatically instantiate appropriate model classes, tokenizers, and image processors based on configuration or model type, supporting both local and remote code execution with trust verification. 2. Key Features: Lazy loading and mapping of model components with fallback mecha",
      "preview_md": "## Task\n**Task Statement: Transformers Auto-Factory and Component Loading System**\n\nDevelop a dynamic model and tokenizer auto-loading system that:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processing_grounding_dino.3d24a7e6.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processing_owlvit.56ec5dc9.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_owlvit.56ec5dc9.lv1",
      "title": "test_processing_owlvit.56ec5dc9.lv1",
      "preview": "Image Preprocessing for Vision-Language Object Detection Core Functionality: Implement image preprocessing pipeline for the OwlViT (Owl Vision Transformer) model that transforms raw images into model-ready tensor format through standardized computer vision operations. Main Features & Requirements: Apply configurable image transformations: resize, center crop, rescale pixel values, and normalize with mean/std Handle s",
      "preview_md": "## Task\n**Task Statement: Image Preprocessing for Vision-Language Object Detection**\n\n**Core Functionality:**\nImplement image preprocessing pipeline for the OwlViT (Owl Vision Transformer) model that transforms raw images into model-ready tensor format through standardized computer vision operations.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processing_owlvit.56ec5dc9.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processing_samhq.7732c356.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_samhq.7732c356.lv1",
      "title": "test_processing_samhq.7732c356.lv1",
      "preview": "SAM Image Preprocessing for Segmentation Implement image preprocessing functionality for the Segment Anything Model (SAM) that handles both input images and segmentation masks. The core requirements include: 1. Image Processing Pipeline: Resize images while maintaining aspect ratio, rescale pixel values, normalize using ImageNet statistics, and pad to fixed dimensions 2. Mask Processing: Apply similar transformations",
      "preview_md": "## Task\n**Task Statement: SAM Image Preprocessing for Segmentation**\n\nImplement image preprocessing functionality for the Segment Anything Model (SAM) that handles both input images and segmentation masks. The core requirements include:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processing_samhq.7732c356.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processing_wav2vec2.4f660c78.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_wav2vec2.4f660c78.lv1",
      "title": "test_processing_wav2vec2.4f660c78.lv1",
      "preview": "Automatic Tokenizer Management and Speech-to-Text Tokenization Core Functionalities: Automatically instantiate appropriate tokenizer classes from pretrained models based on model configuration Provide specialized tokenization for speech-to-text models (Wav2Vec2) with CTC decoding capabilities Handle dynamic tokenizer class resolution and vocabulary management Main Features & Requirements: Auto-detection of tokenizer ",
      "preview_md": "## Task\n**Task Statement: Automatic Tokenizer Management and Speech-to-Text Tokenization**\n\n**Core Functionalities:**\n- Automatically instantiate appropriate tokenizer classes from pretrained models based on model configuration\n- Provide specialized tokenization for speech-to-text models (Wav2Vec2) with CTC decoding capabilities",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processing_wav2vec2.4f660c78.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_processor_auto.d9626018.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processor_auto.d9626018.lv1",
      "title": "test_processor_auto.d9626018.lv1",
      "preview": "Implement and test transformer model component interfaces for tokenization, processing, and configuration management. The task involves creating functionality for: 1. Core functionalities: Vocabulary loading, automatic model component instantiation, dynamic registration of tokenizers/processors, and model configuration initialization 2. Main features: Support for multiple model types (BERT, Wav2Vec2, ConvNext), pretr",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement and test transformer model component interfaces for tokenization, processing, and configuration management. The task involves creating functionality for:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_processor_auto.d9626018.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_serve.4e7860c7.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_serve.4e7860c7.lv1",
      "title": "test_serve.4e7860c7.lv1",
      "preview": "OpenAI-Compatible Model Serving Interface Implement a streaming chat completion service that provides OpenAI-compatible API endpoints for serving transformer models. The core functionalities include: 1. Model Management: Dynamic loading/unloading of models with timeout-based memory management 2. Chat Completion Streaming: Generate real-time streaming responses for chat conversations with support for tool calls and mu",
      "preview_md": "## Task\n**Task Statement: OpenAI-Compatible Model Serving Interface**\n\nImplement a streaming chat completion service that provides OpenAI-compatible API endpoints for serving transformer models. The core functionalities include:\n\n1. **Model Management**: Dynamic loading/unloading of models with timeout-based memory management",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_serve.4e7860c7.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_tests_fetcher.e1abe0dd.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_tests_fetcher.e1abe0dd.lv1",
      "title": "test_tests_fetcher.e1abe0dd.lv1",
      "preview": "Intelligent Test Selection and Dependency Analysis System Core Functionalities: Analyze code changes in a Git repository to identify modified files and their dependencies Build reverse dependency maps to determine which tests are impacted by specific code changes Intelligently filter and select only relevant tests to run based on file modifications Main Features & Requirements: Parse Git diffs and commit messages to ",
      "preview_md": "## Task\n**Task Statement: Intelligent Test Selection and Dependency Analysis System**\n\n**Core Functionalities:**\n- Analyze code changes in a Git repository to identify modified files and their dependencies\n- Build reverse dependency maps to determine which tests are impacted by specific code changes",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_tests_fetcher.e1abe0dd.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_tokenization_gpt_sw3.18420a43.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_tokenization_gpt_sw3.18420a43.lv1",
      "title": "test_tokenization_gpt_sw3.18420a43.lv1",
      "preview": "Special Tokens and Text Processing for Tokenizers Implement functionality for managing special tokens in tokenizer systems, including: 1. Special Token Management: Handle dynamic attribute access and modification of special tokens (bos, eos, unk, sep, pad, cls, mask tokens) with automatic ID conversion and validation 2. Token Collection and Mapping: Provide comprehensive access to all special tokens in both string an",
      "preview_md": "## Task\n**Task Statement: Special Tokens and Text Processing for Tokenizers**\n\nImplement functionality for managing special tokens in tokenizer systems, including:\n\n1. **Special Token Management**: Handle dynamic attribute access and modification of special tokens (bos, eos, unk, sep, pad, cls, mask tokens) with automatic ID conversion and validation",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_tokenization_gpt_sw3.18420a43.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_tokenization_prophetnet.ca02e775.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_tokenization_prophetnet.ca02e775.lv1",
      "title": "test_tokenization_prophetnet.ca02e775.lv1",
      "preview": "Text Character Classification and WordPiece Tokenization Implement character classification utilities and WordPiece tokenization functionality for natural language processing. The core requirements include: 1. Character Classification Functions: Create utilities to identify and categorize Unicode characters as whitespace, control characters, or punctuation marks based on Unicode categories and ASCII ranges. 2. WordPi",
      "preview_md": "## Task\n**Task Statement: Text Character Classification and WordPiece Tokenization**\n\nImplement character classification utilities and WordPiece tokenization functionality for natural language processing. The core requirements include:",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_tokenization_prophetnet.ca02e775.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__transformers.e2e8dbed/test_video_processing_auto.c41f8df4.lv1",
      "split": "full",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_video_processing_auto.c41f8df4.lv1",
      "title": "test_video_processing_auto.c41f8df4.lv1",
      "preview": "Dynamic Module Management and Auto Video Processor System Core Functionalities: Dynamically load and manage custom Python modules from local directories or remote repositories (HuggingFace Hub) Automatically instantiate video processor classes based on model configurations Handle module dependencies, relative imports, and code execution with security considerations Main Features & Requirements: Resolve and cache rela",
      "preview_md": "## Task\n**Task Statement: Dynamic Module Management and Auto Video Processor System**\n\n**Core Functionalities:**\n- Dynamically load and manage custom Python modules from local directories or remote repositories (HuggingFace Hub)\n- Automatically instantiate video processor classes based on model configurations",
      "statement_path": "./data/full/huggingface__transformers.e2e8dbed/test_video_processing_auto.c41f8df4.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__trl.02a34777/test_data_utils.827a9d15.lv1",
      "split": "full",
      "repo_ref": "huggingface__trl.02a34777",
      "repo": "huggingface/trl",
      "case": "test_data_utils.827a9d15.lv1",
      "title": "test_data_utils.827a9d15.lv1",
      "preview": "Conversational Dataset Processing and Multimodal Message Handling Core Functionalities: Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal) Apply chat templates to convert structured conversations into tokenizer-ready text Pack and optimize dataset sequences for efficient training Main Features & Requirements: Handle multimodal messages with text and image co",
      "preview_md": "## Task\n**Task Statement: Conversational Dataset Processing and Multimodal Message Handling**\n\n**Core Functionalities:**\n- Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal)\n- Apply chat templates to convert structured conversations into tokenizer-ready text",
      "statement_path": "./data/full/huggingface__trl.02a34777/test_data_utils.827a9d15.lv1/problem_statement.md"
    },
    {
      "id": "full:huggingface__trl.02a34777/test_data_utils.827a9d15.lv2",
      "split": "full",
      "repo_ref": "huggingface__trl.02a34777",
      "repo": "huggingface/trl",
      "case": "test_data_utils.827a9d15.lv2",
      "title": "test_data_utils.827a9d15.lv2",
      "preview": "Conversational Dataset Processing and Multimodal Message Handling Core Functionalities: Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal) Apply chat templates to convert structured conversations into tokenizer-ready text Pack and optimize dataset sequences for efficient training Main Features & Requirements: Handle multimodal messages with text and image co",
      "preview_md": "## Task\n**Task Statement: Conversational Dataset Processing and Multimodal Message Handling**\n\n**Core Functionalities:**\n- Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal)\n- Apply chat templates to convert structured conversations into tokenizer-ready text",
      "statement_path": "./data/full/huggingface__trl.02a34777/test_data_utils.827a9d15.lv2/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_connector.825ef3b1.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_connector.825ef3b1.lv1",
      "title": "test_connector.825ef3b1.lv1",
      "preview": "Implement Lightning Fabric Component Configuration and Quantized Linear Layer Integration Core Functionalities: Configure and initialize Lightning Fabric components (accelerators, strategies, precision plugins, cluster environments) Implement bitsandbytes quantized linear layers with 8-bit and 4-bit precision modes Provide seamless integration between hardware acceleration and model quantization Main Features & Requi",
      "preview_md": "## Task\n**Task Statement: Implement Lightning Fabric Component Configuration and Quantized Linear Layer Integration**\n\n**Core Functionalities:**\n- Configure and initialize Lightning Fabric components (accelerators, strategies, precision plugins, cluster environments)\n- Implement bitsandbytes quantized linear layers with 8-bit and 4-bit precision modes",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_connector.825ef3b1.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_data.12056068.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_data.12056068.lv1",
      "title": "test_data.12056068.lv1",
      "preview": "DataLoader Management and Optimization Utilities Develop a comprehensive data loading utility system that provides: 1. Core Functionalities: DataLoader introspection and validation (dataset type detection, length checking) Dynamic DataLoader reconstruction with custom samplers and configurations Automatic performance optimization (worker count suggestions, epoch-aware sampling) 2. Main Features & Requirements: Suppor",
      "preview_md": "## Task\n**Task Statement: DataLoader Management and Optimization Utilities**\n\nDevelop a comprehensive data loading utility system that provides:\n\n1. **Core Functionalities:**\n   - DataLoader introspection and validation (dataset type detection, length checking)\n   - Dynamic DataLoader reconstruction with custom samplers and configurations",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_data.12056068.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_data.c8b292af.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_data.c8b292af.lv1",
      "title": "test_data.c8b292af.lv1",
      "preview": "DataLoader Management and Optimization Utilities Develop a comprehensive data loading utility system that provides: 1. Core Functionalities: DataLoader introspection and validation (dataset type detection, length checking) Dynamic DataLoader reconstruction with custom samplers and configurations Automatic performance optimization (worker count suggestions, epoch-aware sampling) 2. Main Features & Requirements: Suppor",
      "preview_md": "## Task\n**Task Statement: DataLoader Management and Optimization Utilities**\n\nDevelop a comprehensive data loading utility system that provides:\n\n1. **Core Functionalities:**\n   - DataLoader introspection and validation (dataset type detection, length checking)\n   - Dynamic DataLoader reconstruction with custom samplers and configurations",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_data.c8b292af.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp_integration.61c07610.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_fsdp_integration.61c07610.lv1",
      "title": "test_fsdp_integration.61c07610.lv1",
      "preview": "Distributed Deep Learning Model Setup and Initialization Implement interfaces for setting up and initializing PyTorch models in distributed training environments. The core functionality should provide: 1. Model Setup and Wrapping: Automatically configure models, optimizers, and data loaders for distributed execution across multiple devices/nodes with appropriate strategy-specific wrappers. 2. Device and Precision Man",
      "preview_md": "## Task\n**Task Statement: Distributed Deep Learning Model Setup and Initialization**\n\nImplement interfaces for setting up and initializing PyTorch models in distributed training environments. The core functionality should provide:",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp_integration.61c07610.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp_integration.61c07610.lv2",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_fsdp_integration.61c07610.lv2",
      "title": "test_fsdp_integration.61c07610.lv2",
      "preview": "Distributed Deep Learning Model Setup and Initialization Implement interfaces for setting up and initializing PyTorch models in distributed training environments. The core functionality should provide: 1. Model Setup and Wrapping: Automatically configure models, optimizers, and data loaders for distributed execution across multiple devices/nodes with appropriate strategy-specific wrappers. 2. Device and Precision Man",
      "preview_md": "## Task\n**Task Statement: Distributed Deep Learning Model Setup and Initialization**\n\nImplement interfaces for setting up and initializing PyTorch models in distributed training environments. The core functionality should provide:",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp_integration.61c07610.lv2/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp.744aa3f5.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_fsdp.744aa3f5.lv1",
      "title": "test_fsdp.744aa3f5.lv1",
      "preview": "FSDP Checkpoint Management Implement checkpoint detection and state dictionary context management for Fully Sharded Data Parallel (FSDP) distributed training systems. Core Functionalities: Detect whether a checkpoint path contains sharded model data across multiple files/processes Provide appropriate context managers for retrieving complete model state dictionaries from distributed shards Main Features & Requirements",
      "preview_md": "## Task\n**Task Statement: FSDP Checkpoint Management**\n\nImplement checkpoint detection and state dictionary context management for Fully Sharded Data Parallel (FSDP) distributed training systems.\n\n**Core Functionalities:**\n- Detect whether a checkpoint path contains sharded model data across multiple files/processes",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_fsdp.744aa3f5.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_hooks.34620588.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_hooks.34620588.lv1",
      "title": "test_hooks.34620588.lv1",
      "preview": "Implement PyTorch Lightning Trainer Interface Components Core Functionalities: Develop a machine learning training orchestration system that manages the complete model lifecycle including training, validation, testing, and prediction phases with comprehensive monitoring and checkpointing capabilities. Main Features & Requirements: Training Loop Management: Execute full training workflows with configurable epochs, ste",
      "preview_md": "## Task\n**Task Statement: Implement PyTorch Lightning Trainer Interface Components**\n\n**Core Functionalities:**\nDevelop a machine learning training orchestration system that manages the complete model lifecycle including training, validation, testing, and prediction phases with comprehensive monitoring and checkpointing capabilities.",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_hooks.34620588.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_model_parallel.7dec8f24.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_model_parallel.7dec8f24.lv1",
      "title": "test_model_parallel.7dec8f24.lv1",
      "preview": "FSDP Checkpoint Format Detection Implement a function to detect whether a given checkpoint path represents a sharded FSDP (Fully Sharded Data Parallel) checkpoint format versus other checkpoint formats. Core Functionality: Analyze checkpoint directory structure to identify sharded FSDP checkpoints Distinguish between sharded and full checkpoint formats Support checkpoint format validation for distributed training wor",
      "preview_md": "## Task\n**Task Statement: FSDP Checkpoint Format Detection**\n\nImplement a function to detect whether a given checkpoint path represents a sharded FSDP (Fully Sharded Data Parallel) checkpoint format versus other checkpoint formats.\n\n**Core Functionality:**\n- Analyze checkpoint directory structure to identify sharded FSDP checkpoints",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_model_parallel.7dec8f24.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_seed.1eaa1d46.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_seed.1eaa1d46.lv1",
      "title": "test_seed.1eaa1d46.lv1",
      "preview": "Random Seed Management for Reproducible Machine Learning Implement a comprehensive seed management system that ensures reproducible random number generation across multiple libraries (PyTorch, NumPy, Python random) and distributed computing environments. The system must provide global seed initialization, seed restoration capabilities, and proper worker process seeding for data loaders in multi-process training scena",
      "preview_md": "## Task\n**Task Statement: Random Seed Management for Reproducible Machine Learning**",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_seed.1eaa1d46.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_seed.1eaa1d46.lv2",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_seed.1eaa1d46.lv2",
      "title": "test_seed.1eaa1d46.lv2",
      "preview": "Random Seed Management for Reproducible Machine Learning Implement a comprehensive seed management system that ensures reproducible random number generation across multiple libraries (PyTorch, NumPy, Python random) and distributed computing environments. The system must provide global seed initialization, seed restoration capabilities, and proper worker process seeding for data loaders in multi-process training scena",
      "preview_md": "## Task\n**Task Statement: Random Seed Management for Reproducible Machine Learning**",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_seed.1eaa1d46.lv2/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_wrappers.d88ce6f6.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_wrappers.d88ce6f6.lv1",
      "title": "test_wrappers.d88ce6f6.lv1",
      "preview": "Implement Fabric Framework Wrapper Components Create wrapper classes and utilities for a distributed machine learning framework that provides: 1. Core Functionalities: Wrap PyTorch components (DataLoader, Optimizer, Module) with framework-specific behavior Enable transparent device management and distributed training capabilities Provide seamless integration while preserving original object interfaces 2. Main Feature",
      "preview_md": "## Task\n**Task Statement: Implement Fabric Framework Wrapper Components**\n\nCreate wrapper classes and utilities for a distributed machine learning framework that provides:\n\n1. **Core Functionalities:**\n   - Wrap PyTorch components (DataLoader, Optimizer, Module) with framework-specific behavior",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_wrappers.d88ce6f6.lv1/problem_statement.md"
    },
    {
      "id": "full:Lightning-AI__pytorch-lightning.126fa6f1/test_xla.7c94e7d2.lv1",
      "split": "full",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_xla.7c94e7d2.lv1",
      "title": "test_xla.7c94e7d2.lv1",
      "preview": "PyTorch Lightning Trainer Configuration and Hardware Management Implement a machine learning trainer system that provides: 1. Core Functionality: Initialize and configure a training orchestrator that manages the complete ML workflow (training, validation, testing, prediction) with extensive customization options for training parameters, data handling, and execution control. 2. Hardware Abstraction: Provide unified ac",
      "preview_md": "## Task\n**Task Statement: PyTorch Lightning Trainer Configuration and Hardware Management**\n\nImplement a machine learning trainer system that provides:",
      "statement_path": "./data/full/Lightning-AI__pytorch-lightning.126fa6f1/test_xla.7c94e7d2.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_fused_add_rms_norm.fecadbdf.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_fused_add_rms_norm.fecadbdf.lv1",
      "title": "test_fused_add_rms_norm.fecadbdf.lv1",
      "preview": "Implement Fused Add-RMS Normalization Operation Develop a high-performance fused kernel that combines element-wise tensor addition with RMS (Root Mean Square) normalization in a single operation. The implementation should optimize memory usage and computational efficiency by eliminating intermediate tensor storage, support configurable precision casting modes, provide optional in-place computation, and handle various",
      "preview_md": "## Task\n**Task Statement: Implement Fused Add-RMS Normalization Operation**",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_fused_add_rms_norm.fecadbdf.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_fused_linear_jsd.4bd46945.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_fused_linear_jsd.4bd46945.lv1",
      "title": "test_fused_linear_jsd.4bd46945.lv1",
      "preview": "Implement Fused Linear Jensen-Shannon Divergence for Knowledge Distillation Core Functionality: Develop a memory-efficient fused operation that combines linear transformation with Jensen-Shannon Divergence (JSD) loss computation for knowledge distillation between student and teacher models. Main Features & Requirements: Fuse linear layer computation with JSD loss calculation in a single kernel operation Support stude",
      "preview_md": "## Task\n**Task Statement: Implement Fused Linear Jensen-Shannon Divergence for Knowledge Distillation**\n\n**Core Functionality:**\nDevelop a memory-efficient fused operation that combines linear transformation with Jensen-Shannon Divergence (JSD) loss computation for knowledge distillation between student and teacher models.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_fused_linear_jsd.4bd46945.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_fused_neighborhood_attention.78217be4.lv2",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_fused_neighborhood_attention.78217be4.lv2",
      "title": "test_fused_neighborhood_attention.78217be4.lv2",
      "preview": "Implement Fused Neighborhood Attention Mechanism Develop an efficient neighborhood attention system that restricts attention computation to local windows around each position, reducing complexity from O(n) to O(n*k). The implementation should provide: Core Functionalities: Localized attention computation within configurable neighborhood windows Fused operations combining query-key-value projections with attention ca",
      "preview_md": "## Task\n**Task Statement: Implement Fused Neighborhood Attention Mechanism**\n\nDevelop an efficient neighborhood attention system that restricts attention computation to local windows around each position, reducing complexity from O(n) to O(n*k). The implementation should provide:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_fused_neighborhood_attention.78217be4.lv2/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_layer_norm.ed322799.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_layer_norm.ed322799.lv1",
      "title": "test_layer_norm.ed322799.lv1",
      "preview": "Implement Optimized Layer Normalization Function Develop a high-performance layer normalization function that: 1. Core Functionality: Applies layer normalization to input tensors using weight, bias, and epsilon parameters for numerical stability 2. Key Requirements: Integrate with existing Liger kernel optimization framework Support standard layer normalization mathematical operations (mean centering, variance scalin",
      "preview_md": "## Task\n**Task Statement: Implement Optimized Layer Normalization Function**\n\nDevelop a high-performance layer normalization function that:\n\n1. **Core Functionality**: Applies layer normalization to input tensors using weight, bias, and epsilon parameters for numerical stability\n\n2. **Key Requirements**:",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_layer_norm.ed322799.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_mini_models.96209d55.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_mini_models.96209d55.lv1",
      "title": "test_mini_models.96209d55.lv1",
      "preview": "Implement Optimized Forward Pass Functions for Large Language Models Core Functionalities: Implement memory-efficient forward pass methods for various transformer-based language models (LLaMA, Qwen, Phi3, GLM4, etc.) Replace standard cross-entropy loss computation with fused linear cross-entropy operations Apply rotary positional embeddings (RoPE) to query and key tensors Main Features & Requirements: Support both te",
      "preview_md": "## Task\n**Task Statement: Implement Optimized Forward Pass Functions for Large Language Models**\n\n**Core Functionalities:**\n- Implement memory-efficient forward pass methods for various transformer-based language models (LLaMA, Qwen, Phi3, GLM4, etc.)\n- Replace standard cross-entropy loss computation with fused linear cross-entropy operations",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_mini_models.96209d55.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_multi_token_attention.4c06a35b.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_multi_token_attention.4c06a35b.lv1",
      "title": "test_multi_token_attention.4c06a35b.lv1",
      "preview": "Implement Multi-Token Attention with Convolution-Based Processing Core Functionality: Develop a multi-token attention mechanism that applies convolution operations to attention scores, enabling localized feature extraction across token sequences. Main Features & Requirements: Process 4D attention score tensors (batch, channels, length, length) using 2D convolution Support configurable convolution parameters (stride, ",
      "preview_md": "## Task\n**Task Statement: Implement Multi-Token Attention with Convolution-Based Processing**\n\n**Core Functionality:**\nDevelop a multi-token attention mechanism that applies convolution operations to attention scores, enabling localized feature extraction across token sequences.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_multi_token_attention.4c06a35b.lv1/problem_statement.md"
    },
    {
      "id": "full:linkedin__Liger-Kernel.c856fbab/test_poly_norm.7b0e3399.lv1",
      "split": "full",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_poly_norm.7b0e3399.lv1",
      "title": "test_poly_norm.7b0e3399.lv1",
      "preview": "Implement Polynomial Normalization Function Develop a polynomial normalization operation that applies advanced normalization techniques to input tensors using learnable weight and bias parameters. The function should support configurable epsilon values for numerical stability and optional in-place computation for memory efficiency. Key considerations include maintaining numerical precision, supporting gradient comput",
      "preview_md": "## Task\n**Task Statement: Implement Polynomial Normalization Function**",
      "statement_path": "./data/full/linkedin__Liger-Kernel.c856fbab/test_poly_norm.7b0e3399.lv1/problem_statement.md"
    },
    {
      "id": "full:matplotlib__matplotlib.86a476d2/test_angle_helper.48e96b5a.lv2",
      "split": "full",
      "repo_ref": "matplotlib__matplotlib.86a476d2",
      "repo": "matplotlib/matplotlib",
      "case": "test_angle_helper.48e96b5a.lv2",
      "title": "test_angle_helper.48e96b5a.lv2",
      "preview": "Angular Axis Tick Selection and Formatting Develop functions to automatically determine optimal tick intervals and format labels for angular coordinate systems (degrees and hours). The core functionality should: 1. Smart Step Selection: Calculate appropriate tick intervals based on value ranges, supporting both degree (0-360) and hour (0-24h) systems with hierarchical precision (degrees/hours, minutes, seconds) 2. A",
      "preview_md": "## Task\n**Task Statement: Angular Axis Tick Selection and Formatting**\n\nDevelop functions to automatically determine optimal tick intervals and format labels for angular coordinate systems (degrees and hours). The core functionality should:",
      "statement_path": "./data/full/matplotlib__matplotlib.86a476d2/test_angle_helper.48e96b5a.lv2/problem_statement.md"
    },
    {
      "id": "full:matplotlib__matplotlib.86a476d2/test_backend_registry.872ba384.lv1",
      "split": "full",
      "repo_ref": "matplotlib__matplotlib.86a476d2",
      "repo": "matplotlib/matplotlib",
      "case": "test_backend_registry.872ba384.lv1",
      "title": "test_backend_registry.872ba384.lv1",
      "preview": "Matplotlib Backend Registry Management Develop a centralized registry system that manages and resolves Matplotlib rendering backends across different GUI frameworks and output formats. Core Functionalities: Maintain a comprehensive registry of built-in, external, and dynamically-loaded backends Map backends to their corresponding GUI frameworks (Qt, Tk, GTK, etc.) or headless modes Resolve backend names and GUI frame",
      "preview_md": "## Task\n**Task Statement: Matplotlib Backend Registry Management**\n\nDevelop a centralized registry system that manages and resolves Matplotlib rendering backends across different GUI frameworks and output formats.\n\n**Core Functionalities:**\n- Maintain a comprehensive registry of built-in, external, and dynamically-loaded backends",
      "statement_path": "./data/full/matplotlib__matplotlib.86a476d2/test_backend_registry.872ba384.lv1/problem_statement.md"
    },
    {
      "id": "full:mesonbuild__meson.f5d81d07/cargotests.8e49c2d0.lv1",
      "split": "full",
      "repo_ref": "mesonbuild__meson.f5d81d07",
      "repo": "mesonbuild/meson",
      "case": "cargotests.8e49c2d0.lv1",
      "title": "cargotests.8e49c2d0.lv1",
      "preview": "Cargo-to-Meson Build System Integration Core Functionalities: Parse and interpret Cargo manifest files (Cargo.toml, Cargo.lock) and convert them to Meson build definitions Handle Rust package dependencies, workspace management, and feature configuration Process Cargo configuration expressions and version constraints for cross-platform compatibility Main Features & Requirements: Load and validate TOML configuration fi",
      "preview_md": "## Task\n**Task Statement: Cargo-to-Meson Build System Integration**\n\n**Core Functionalities:**\n- Parse and interpret Cargo manifest files (Cargo.toml, Cargo.lock) and convert them to Meson build definitions\n- Handle Rust package dependencies, workspace management, and feature configuration",
      "statement_path": "./data/full/mesonbuild__meson.f5d81d07/cargotests.8e49c2d0.lv1/problem_statement.md"
    },
    {
      "id": "full:mesonbuild__meson.f5d81d07/cargotests.8e49c2d0.lv2",
      "split": "full",
      "repo_ref": "mesonbuild__meson.f5d81d07",
      "repo": "mesonbuild/meson",
      "case": "cargotests.8e49c2d0.lv2",
      "title": "cargotests.8e49c2d0.lv2",
      "preview": "Cargo-to-Meson Build System Integration Core Functionalities: Parse and interpret Cargo manifest files (Cargo.toml, Cargo.lock) and convert them to Meson build definitions Handle Rust package dependencies, workspace management, and feature configuration Process Cargo configuration expressions and version constraints for cross-platform compatibility Main Features & Requirements: Load and validate TOML configuration fi",
      "preview_md": "## Task\n**Task Statement: Cargo-to-Meson Build System Integration**\n\n**Core Functionalities:**\n- Parse and interpret Cargo manifest files (Cargo.toml, Cargo.lock) and convert them to Meson build definitions\n- Handle Rust package dependencies, workspace management, and feature configuration",
      "statement_path": "./data/full/mesonbuild__meson.f5d81d07/cargotests.8e49c2d0.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_abstract_store.e5ff5123.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_abstract_store.e5ff5123.lv1",
      "title": "test_abstract_store.e5ff5123.lv1",
      "preview": "Implement Bulk Metric History Retrieval with Step-based Sampling Core Functionalities: Retrieve metric history data across multiple ML experiment runs within specified step ranges Sample metric data points intelligently to limit result size while preserving data boundaries Support both step-based filtering and cross-run metric aggregation Main Features & Requirements: Fetch metrics for specific steps across runs (get",
      "preview_md": "## Task\n**Task Statement: Implement Bulk Metric History Retrieval with Step-based Sampling**\n\n**Core Functionalities:**\n- Retrieve metric history data across multiple ML experiment runs within specified step ranges\n- Sample metric data points intelligently to limit result size while preserving data boundaries",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_abstract_store.e5ff5123.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_ai_command_utils.85dcb487.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_ai_command_utils.85dcb487.lv1",
      "title": "test_ai_command_utils.85dcb487.lv1",
      "preview": "MLflow AI Command Management System Build a command management system that can discover, parse, and retrieve AI commands stored as markdown files with YAML frontmatter. The system should provide functionality to: 1. Core Features: Parse YAML metadata from markdown files, list available commands with filtering capabilities, and retrieve command content 2. Key Requirements: Handle file system operations across differen",
      "preview_md": "## Task\n**Task Statement: MLflow AI Command Management System**\n\nBuild a command management system that can discover, parse, and retrieve AI commands stored as markdown files with YAML frontmatter. The system should provide functionality to:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_ai_command_utils.85dcb487.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_ai_command_utils.85dcb487.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_ai_command_utils.85dcb487.lv2",
      "title": "test_ai_command_utils.85dcb487.lv2",
      "preview": "MLflow AI Command Management System Build a command management system that can discover, parse, and retrieve AI commands stored as markdown files with YAML frontmatter. The system should provide functionality to: 1. Core Features: Parse YAML metadata from markdown files, list available commands with filtering capabilities, and retrieve command content 2. Key Requirements: Handle file system operations across differen",
      "preview_md": "## Task\n**Task Statement: MLflow AI Command Management System**\n\nBuild a command management system that can discover, parse, and retrieve AI commands stored as markdown files with YAML frontmatter. The system should provide functionality to:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_ai_command_utils.85dcb487.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_assessment.df512d1b.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_assessment.df512d1b.lv1",
      "title": "test_assessment.df512d1b.lv1",
      "preview": "Implement MLflow Distributed Tracing System Core Functionalities: Create and manage distributed trace spans with hierarchical parent-child relationships Capture, store, and retrieve trace execution data including inputs, outputs, attributes, and events Enable/disable tracing capabilities dynamically across different execution contexts Export traces to multiple destinations (MLflow experiments, OTLP endpoints, UC tabl",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Distributed Tracing System**\n\n**Core Functionalities:**\n- Create and manage distributed trace spans with hierarchical parent-child relationships\n- Capture, store, and retrieve trace execution data including inputs, outputs, attributes, and events",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_assessment.df512d1b.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_bedrock_autolog.f008b521.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_bedrock_autolog.f008b521.lv1",
      "title": "test_bedrock_autolog.f008b521.lv1",
      "preview": "Implement MLflow Tracing and Autologging Infrastructure Core Functionalities: Build a distributed tracing system for ML workflows that captures execution traces, spans, and metadata across different storage backends (experiments, Unity Catalog tables) Implement automatic logging (autologging) framework that safely patches ML library functions to capture training metrics, parameters, and artifacts without disrupting u",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Tracing and Autologging Infrastructure**\n\n**Core Functionalities:**\n- Build a distributed tracing system for ML workflows that captures execution traces, spans, and metadata across different storage backends (experiments, Unity Catalog tables)",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_bedrock_autolog.f008b521.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_client_webhooks.75e39b52.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_client_webhooks.75e39b52.lv1",
      "title": "test_client_webhooks.75e39b52.lv1",
      "preview": "Implement MLflow Webhook Management System Core Functionalities: Create, retrieve, update, and delete webhooks for MLflow model registry events Configure webhook endpoints with event triggers, authentication, and status management Test webhook functionality and handle webhook lifecycle operations Main Features & Requirements: Support webhook registration with configurable URLs, event types, and optional secrets Imple",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Webhook Management System**\n\n**Core Functionalities:**\n- Create, retrieve, update, and delete webhooks for MLflow model registry events\n- Configure webhook endpoints with event triggers, authentication, and status management\n- Test webhook functionality and handle webhook lifecycle operations",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_client_webhooks.75e39b52.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_client_webhooks.75e39b52.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_client_webhooks.75e39b52.lv2",
      "title": "test_client_webhooks.75e39b52.lv2",
      "preview": "Implement MLflow Webhook Management System Core Functionalities: Create, retrieve, update, and delete webhooks for MLflow model registry events Configure webhook endpoints with event triggers, authentication, and status management Test webhook functionality and handle webhook lifecycle operations Main Features & Requirements: Support webhook registration with configurable URLs, event types, and optional secrets Imple",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Webhook Management System**\n\n**Core Functionalities:**\n- Create, retrieve, update, and delete webhooks for MLflow model registry events\n- Configure webhook endpoints with event triggers, authentication, and status management\n- Test webhook functionality and handle webhook lifecycle operations",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_client_webhooks.75e39b52.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_client.42de781c.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_client.42de781c.lv1",
      "title": "test_client.42de781c.lv1",
      "preview": "Implement a Telemetry Data Collection and Transmission System Core Functionalities: Collect, batch, and asynchronously transmit telemetry records to remote servers Manage telemetry client lifecycle with proper initialization, activation, and cleanup Handle configuration fetching and event filtering based on server-side rules Main Features & Requirements: Thread-safe record queuing with configurable batch sizes and ti",
      "preview_md": "## Task\n**Task Statement: Implement a Telemetry Data Collection and Transmission System**\n\n**Core Functionalities:**\n- Collect, batch, and asynchronously transmit telemetry records to remote servers\n- Manage telemetry client lifecycle with proper initialization, activation, and cleanup",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_client.42de781c.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_config.c63d41b0.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_config.c63d41b0.lv1",
      "title": "test_config.c63d41b0.lv1",
      "preview": "Configuration Management for MLflow-Claude Integration Develop a configuration management system that enables seamless integration between MLflow tracking and Claude AI development environment. The system should provide persistent configuration storage, environment variable management, and tracing status monitoring. Core Functionalities: Load/save configuration data to/from Claude settings files Manage MLflow environ",
      "preview_md": "## Task\n**Task Statement: Configuration Management for MLflow-Claude Integration**\n\nDevelop a configuration management system that enables seamless integration between MLflow tracking and Claude AI development environment. The system should provide persistent configuration storage, environment variable management, and tracing status monitoring.",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_config.c63d41b0.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_config.c63d41b0.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_config.c63d41b0.lv2",
      "title": "test_config.c63d41b0.lv2",
      "preview": "Configuration Management for MLflow-Claude Integration Develop a configuration management system that enables seamless integration between MLflow tracking and Claude AI development environment. The system should provide persistent configuration storage, environment variable management, and tracing status monitoring. Core Functionalities: Load/save configuration data to/from Claude settings files Manage MLflow environ",
      "preview_md": "## Task\n**Task Statement: Configuration Management for MLflow-Claude Integration**\n\nDevelop a configuration management system that enables seamless integration between MLflow tracking and Claude AI development environment. The system should provide persistent configuration storage, environment variable management, and tracing status monitoring.",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_config.c63d41b0.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_databricks_adapter.3b72f337.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_databricks_adapter.3b72f337.lv1",
      "title": "test_databricks_adapter.3b72f337.lv1",
      "preview": "Databricks AI Model Judge Integration Develop interfaces for integrating Databricks AI models as automated judges in ML evaluation workflows. The system should: Core Functionalities: Invoke Databricks models via managed services and direct REST endpoints Parse and validate model responses into structured feedback objects Handle chat completion requests with system/user prompt separation Key Features: Robust error han",
      "preview_md": "## Task\n**Task Statement: Databricks AI Model Judge Integration**\n\nDevelop interfaces for integrating Databricks AI models as automated judges in ML evaluation workflows. The system should:\n\n**Core Functionalities:**\n- Invoke Databricks models via managed services and direct REST endpoints\n- Parse and validate model responses into structured feedback objects",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_databricks_adapter.3b72f337.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_databricks_rest_store.50525c62.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_databricks_rest_store.50525c62.lv1",
      "title": "test_databricks_rest_store.50525c62.lv1",
      "preview": "Protocol Buffer to JSON Conversion and Tracing Data Serialization Core Functionalities: Convert Protocol Buffer messages to JSON format while preserving data type integrity (especially int64 fields) Serialize MLflow tracing entities (traces, spans, assessments) to Protocol Buffer format Handle bidirectional conversion between different data representation formats Main Features & Requirements: Maintain proper int64 fi",
      "preview_md": "## Task\n**Task Statement: Protocol Buffer to JSON Conversion and Tracing Data Serialization**\n\n**Core Functionalities:**\n- Convert Protocol Buffer messages to JSON format while preserving data type integrity (especially int64 fields)\n- Serialize MLflow tracing entities (traces, spans, assessments) to Protocol Buffer format",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_databricks_rest_store.50525c62.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_databricks_tracing_utils.8ef44eb4.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_databricks_tracing_utils.8ef44eb4.lv1",
      "title": "test_databricks_tracing_utils.8ef44eb4.lv1",
      "preview": "MLflow Tracing and Assessment Data Management Core Functionalities: Implement data serialization/deserialization between MLflow entities and protocol buffer formats for distributed tracing systems Manage span lifecycle operations including input/output handling and attribute management Handle assessment source type validation and standardization with backward compatibility Main Features & Requirements: Convert trace ",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing and Assessment Data Management**\n\n**Core Functionalities:**\n- Implement data serialization/deserialization between MLflow entities and protocol buffer formats for distributed tracing systems\n- Manage span lifecycle operations including input/output handling and attribute management",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_databricks_tracing_utils.8ef44eb4.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_databricks.cbc54e8c.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_databricks.cbc54e8c.lv1",
      "title": "test_databricks.cbc54e8c.lv1",
      "preview": "HTTP Client and Deployment Management System Core Functionalities: Implement robust HTTP request handling with configurable retry mechanisms, authentication, and timeout management Provide deployment client factory and management for ML model deployment across different target platforms Validate deployment configuration parameters to ensure proper timeout relationships Main Features & Requirements: Support multiple a",
      "preview_md": "## Task\n**Task Statement: HTTP Client and Deployment Management System**\n\n**Core Functionalities:**\n- Implement robust HTTP request handling with configurable retry mechanisms, authentication, and timeout management\n- Provide deployment client factory and management for ML model deployment across different target platforms",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_databricks.cbc54e8c.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_dependencies_schema.7019486f.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_dependencies_schema.7019486f.lv1",
      "title": "test_dependencies_schema.7019486f.lv1",
      "preview": "MLflow Model Dependencies Schema Management Develop a system to manage and configure schema definitions for model dependencies, specifically focusing on retriever components in generative AI applications. The system should: Core Functionalities: Define and store schema configurations for retriever tools that specify data structure mappings (primary keys, text columns, document URIs) Serialize/deserialize schema defin",
      "preview_md": "## Task\n**Task Statement: MLflow Model Dependencies Schema Management**\n\nDevelop a system to manage and configure schema definitions for model dependencies, specifically focusing on retriever components in generative AI applications. The system should:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_dependencies_schema.7019486f.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_evaluation_dataset.4a6528c2.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_evaluation_dataset.4a6528c2.lv1",
      "title": "test_evaluation_dataset.4a6528c2.lv1",
      "preview": "Dataset Identification and Tracking Interface Implement a dataset management system that provides unique identification and naming capabilities for ML datasets. The core functionality involves: 1. Dataset Identification: Generate and maintain unique identifiers (digests/hashes) for datasets to enable version tracking and change detection 2. Dataset Naming: Provide human-readable names for datasets with fallback defau",
      "preview_md": "## Task\n**Task Statement: Dataset Identification and Tracking Interface**\n\nImplement a dataset management system that provides unique identification and naming capabilities for ML datasets. The core functionality involves:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_evaluation_dataset.4a6528c2.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_file_store_logged_model.a9596c54.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_file_store_logged_model.a9596c54.lv1",
      "title": "test_file_store_logged_model.a9596c54.lv1",
      "preview": "MLflow Tracking System Implementation Core Functionalities: Implement a comprehensive machine learning experiment tracking system that manages experiments, runs, models, datasets, and traces with support for multiple storage backends (file-based and database-backed). Main Features & Requirements: Entity Management: Create, read, update, and delete operations for experiments, runs, logged models, datasets, and traces ",
      "preview_md": "## Task\n**Task Statement: MLflow Tracking System Implementation**\n\n**Core Functionalities:**\nImplement a comprehensive machine learning experiment tracking system that manages experiments, runs, models, datasets, and traces with support for multiple storage backends (file-based and database-backed).\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_file_store_logged_model.a9596c54.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_file_store_logged_model.a9596c54.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_file_store_logged_model.a9596c54.lv2",
      "title": "test_file_store_logged_model.a9596c54.lv2",
      "preview": "MLflow Tracking System Implementation Core Functionalities: Implement a comprehensive machine learning experiment tracking system that manages experiments, runs, models, datasets, and traces with support for multiple storage backends (file-based and database-backed). Main Features & Requirements: Entity Management: Create, read, update, and delete operations for experiments, runs, logged models, datasets, and traces ",
      "preview_md": "## Task\n**Task Statement: MLflow Tracking System Implementation**\n\n**Core Functionalities:**\nImplement a comprehensive machine learning experiment tracking system that manages experiments, runs, models, datasets, and traces with support for multiple storage backends (file-based and database-backed).\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_file_store_logged_model.a9596c54.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_genai_eval_utils.2d5441f2.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_genai_eval_utils.2d5441f2.lv1",
      "title": "test_genai_eval_utils.2d5441f2.lv1",
      "preview": "GenAI Evaluation Results Processing and Display Core Functionalities: Process and format evaluation results from generative AI model assessments Resolve scorer configurations from built-in and registered evaluation metrics Extract structured assessment data from evaluation DataFrames and format for tabular display Main Features & Requirements: Support multiple scorer types (built-in scorers via class/snake_case names",
      "preview_md": "## Task\n**Task Statement: GenAI Evaluation Results Processing and Display**\n\n**Core Functionalities:**\n- Process and format evaluation results from generative AI model assessments\n- Resolve scorer configurations from built-in and registered evaluation metrics\n- Extract structured assessment data from evaluation DataFrames and format for tabular display",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_genai_eval_utils.2d5441f2.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_genai_metrics.5f107830.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_genai_metrics.5f107830.lv1",
      "title": "test_genai_metrics.5f107830.lv1",
      "preview": "GenAI Model Evaluation Framework Create a comprehensive evaluation system for generative AI models that enables: 1. Core Functionalities: Define and execute custom evaluation metrics using LLM-as-a-judge methodology Format and template prompts for consistent evaluation scoring Extract structured scores and justifications from LLM responses Aggregate evaluation results across multiple data points 2. Main Features & Re",
      "preview_md": "## Task\n**Task Statement: GenAI Model Evaluation Framework**\n\nCreate a comprehensive evaluation system for generative AI models that enables:\n\n1. **Core Functionalities:**\n   - Define and execute custom evaluation metrics using LLM-as-a-judge methodology\n   - Format and template prompts for consistent evaluation scoring",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_genai_metrics.5f107830.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_git_versioning.b91d0418.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_git_versioning.b91d0418.lv1",
      "title": "test_git_versioning.b91d0418.lv1",
      "preview": "MLflow Model and Git Integration Management Develop functionality to search and manage logged machine learning models with git version control integration. The core objectives include: 1. Model Search and Discovery: Implement search capabilities for logged models across experiments with filtering, ordering, and pagination support 2. Git Version Tracking: Extract and manage git repository information (branch, commit, ",
      "preview_md": "## Task\n**Task Statement: MLflow Model and Git Integration Management**\n\nDevelop functionality to search and manage logged machine learning models with git version control integration. The core objectives include:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_git_versioning.b91d0418.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_jsonpath_utils.eb851c42.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_jsonpath_utils.eb851c42.lv1",
      "title": "test_jsonpath_utils.eb851c42.lv1",
      "preview": "JSON Path Navigation and Filtering Utilities Develop a lightweight JSONPath-like system for navigating and manipulating nested JSON structures without external dependencies. The system should support dot notation path traversal with wildcard expansion, handle special characters in field names using backtick escaping, and provide structure-preserving filtering capabilities. Core Functionalities: Parse and traverse JSO",
      "preview_md": "## Task\n**Task Statement: JSON Path Navigation and Filtering Utilities**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_jsonpath_utils.eb851c42.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_jsonpath_utils.eb851c42.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_jsonpath_utils.eb851c42.lv2",
      "title": "test_jsonpath_utils.eb851c42.lv2",
      "preview": "JSON Path Navigation and Filtering Utilities Develop a lightweight JSONPath-like system for navigating and manipulating nested JSON structures without external dependencies. The system should support dot notation path traversal with wildcard expansion, handle special characters in field names using backtick escaping, and provide structure-preserving filtering capabilities. Core Functionalities: Parse and traverse JSO",
      "preview_md": "## Task\n**Task Statement: JSON Path Navigation and Filtering Utilities**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_jsonpath_utils.eb851c42.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_judge_tool_search_traces.cad8fc4b.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_judge_tool_search_traces.cad8fc4b.lv1",
      "title": "test_judge_tool_search_traces.cad8fc4b.lv1",
      "preview": "Assessment Source Management and Trace Search Tool Implementation Core Functionalities: Standardize and validate assessment source types (human, LLM judge, code-based) with proper deprecation handling Provide trace search capabilities within MLflow experiments with filtering, ordering, and result pagination Convert assessment data between internal formats and tool-compatible types Main Features & Requirements: Suppor",
      "preview_md": "## Task\n**Task Statement: Assessment Source Management and Trace Search Tool Implementation**\n\n**Core Functionalities:**\n- Standardize and validate assessment source types (human, LLM judge, code-based) with proper deprecation handling\n- Provide trace search capabilities within MLflow experiments with filtering, ordering, and result pagination",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_judge_tool_search_traces.cad8fc4b.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_log_image.439a816d.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_log_image.439a816d.lv1",
      "title": "test_log_image.439a816d.lv1",
      "preview": "MLflow Image and Artifact Async Logging Management Implement functionality to log images with flexible storage options (time-stepped tracking vs. artifact files) and manage asynchronous artifact logging operations. The core requirements include: 1. Image Logging Capabilities: Support multiple image formats (numpy arrays, PIL images, MLflow Image objects) with both time-stepped logging for iterative processes and dire",
      "preview_md": "## Task\n**Task Statement: MLflow Image and Artifact Async Logging Management**\n\nImplement functionality to log images with flexible storage options (time-stepped tracking vs. artifact files) and manage asynchronous artifact logging operations. The core requirements include:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_log_image.439a816d.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_mlflow.d1847ff9.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_mlflow.d1847ff9.lv1",
      "title": "test_mlflow.d1847ff9.lv1",
      "preview": "MLflow Deployment and Gateway Interface Implementation Core Functionalities: Implement HTTP-based deployment client interfaces for MLflow that handle model serving endpoints with authentication, rate limiting, and configuration management. Main Features & Requirements: HTTP request handling with retry logic, authentication (Bearer, Basic, AWS SigV4), and timeout management Deployment client factory that returns provi",
      "preview_md": "## Task\n**Task Statement: MLflow Deployment and Gateway Interface Implementation**\n\n**Core Functionalities:**\nImplement HTTP-based deployment client interfaces for MLflow that handle model serving endpoints with authentication, rate limiting, and configuration management.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_mlflow.d1847ff9.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_numpy_dataset.1beaad57.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_numpy_dataset.1beaad57.lv1",
      "title": "test_numpy_dataset.1beaad57.lv1",
      "preview": "MLflow Dataset and Schema Management System Implement a comprehensive dataset and schema management system for MLflow that provides: Core Functionalities: Dataset representation with metadata (name, digest, source) and serialization capabilities Schema inference and validation for various data types (tensors, arrays, objects, primitives) Type-safe data conversion and cleaning utilities Dataset source abstraction for ",
      "preview_md": "## Task\n**Task Statement: MLflow Dataset and Schema Management System**\n\nImplement a comprehensive dataset and schema management system for MLflow that provides:\n\n**Core Functionalities:**\n- Dataset representation with metadata (name, digest, source) and serialization capabilities",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_numpy_dataset.1beaad57.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_numpy_dataset.1beaad57.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_numpy_dataset.1beaad57.lv2",
      "title": "test_numpy_dataset.1beaad57.lv2",
      "preview": "MLflow Dataset and Schema Management System Implement a comprehensive dataset and schema management system for MLflow that provides: Core Functionalities: Dataset representation with metadata (name, digest, source) and serialization capabilities Schema inference and validation for various data types (tensors, arrays, objects, primitives) Type-safe data conversion and cleaning utilities Dataset source abstraction for ",
      "preview_md": "## Task\n**Task Statement: MLflow Dataset and Schema Management System**\n\nImplement a comprehensive dataset and schema management system for MLflow that provides:\n\n**Core Functionalities:**\n- Dataset representation with metadata (name, digest, source) and serialization capabilities",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_numpy_dataset.1beaad57.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_presigned_url_artifact_repo.8d57288e.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_presigned_url_artifact_repo.8d57288e.lv1",
      "title": "test_presigned_url_artifact_repo.8d57288e.lv1",
      "preview": "Protocol Buffer and Artifact Management Utilities Core Functionalities: Convert Protocol Buffer messages to JSON format while preserving data type integrity (especially int64 fields) Implement credential retry mechanisms for artifact repository operations Handle JSON serialization/deserialization with proper type casting and field merging Main Features & Requirements: Maintain numeric precision when converting protob",
      "preview_md": "## Task\n**Task Statement: Protocol Buffer and Artifact Management Utilities**\n\n**Core Functionalities:**\n- Convert Protocol Buffer messages to JSON format while preserving data type integrity (especially int64 fields)\n- Implement credential retry mechanisms for artifact repository operations",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_presigned_url_artifact_repo.8d57288e.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_responses_agent.072c4133.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_responses_agent.072c4133.lv1",
      "title": "test_responses_agent.072c4133.lv1",
      "preview": "MLflow Model Interface Implementation Implement a comprehensive model management system that provides: 1. Core Functionalities: Model lifecycle management (creation, loading, logging, validation) Multi-format model support with flavor-specific configurations Distributed tracing and span management for model operations Type-safe data validation and schema inference Artifact and dependency management 2. Main Features &",
      "preview_md": "## Task\n**Task Statement: MLflow Model Interface Implementation**\n\nImplement a comprehensive model management system that provides:\n\n1. **Core Functionalities:**\n   - Model lifecycle management (creation, loading, logging, validation)\n   - Multi-format model support with flavor-specific configurations",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_responses_agent.072c4133.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_rest_store_webhooks.6d85971c.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_rest_store_webhooks.6d85971c.lv1",
      "title": "test_rest_store_webhooks.6d85971c.lv1",
      "preview": "Implement MLflow Webhook Entity Management System Core Functionalities: Create and manage webhook entities that respond to MLflow events (model registry, prompt management) Handle webhook lifecycle operations including configuration, validation, and testing Support event-driven notifications for ML model and prompt versioning activities Main Features & Requirements: Define webhook properties (ID, name, URL, events, s",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Webhook Entity Management System**\n\n**Core Functionalities:**\n- Create and manage webhook entities that respond to MLflow events (model registry, prompt management)\n- Handle webhook lifecycle operations including configuration, validation, and testing",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_rest_store_webhooks.6d85971c.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_scorer_description.9163195a.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_scorer_description.9163195a.lv1",
      "title": "test_scorer_description.9163195a.lv1",
      "preview": "AI Model Evaluation Framework Develop a flexible evaluation system for AI models and applications that provides: Core Functionalities: Custom scorer creation and execution for evaluating model outputs, inputs, expectations, and execution traces Judge creation with natural language instructions for LLM-based evaluation Automatic scoring with configurable sampling rates and filtering Registration and lifecycle manageme",
      "preview_md": "## Task\n**Task Statement: AI Model Evaluation Framework**\n\nDevelop a flexible evaluation system for AI models and applications that provides:\n\n**Core Functionalities:**\n- Custom scorer creation and execution for evaluating model outputs, inputs, expectations, and execution traces\n- Judge creation with natural language instructions for LLM-based evaluation",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_scorer_description.9163195a.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_scorers.63dc8c7a.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_scorers.63dc8c7a.lv1",
      "title": "test_scorers.63dc8c7a.lv1",
      "preview": "MLflow GenAI Judge and Scorer Management System Core Functionalities: Create and register custom LLM-based judges for evaluating AI model outputs Manage scorer registry with listing and registration capabilities Format evaluation results and metadata for display Main Features & Requirements: Support natural language instruction-based judge creation with template variables (inputs, outputs, expectations, trace) Valida",
      "preview_md": "## Task\n**Task Statement: MLflow GenAI Judge and Scorer Management System**\n\n**Core Functionalities:**\n- Create and register custom LLM-based judges for evaluating AI model outputs\n- Manage scorer registry with listing and registration capabilities\n- Format evaluation results and metadata for display",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_scorers.63dc8c7a.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_security.98fcde1d.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_security.98fcde1d.lv1",
      "title": "test_security.98fcde1d.lv1",
      "preview": "Implement Web Server Security Middleware Build a comprehensive security middleware system for a web server that provides: Core Functionalities: Host header validation to prevent DNS rebinding attacks Cross-Origin Resource Sharing (CORS) protection with configurable origins Security header injection for enhanced protection Main Features & Requirements: Configure allowed hosts and origins from environment variables or ",
      "preview_md": "## Task\n**Task Statement: Implement Web Server Security Middleware**\n\nBuild a comprehensive security middleware system for a web server that provides:\n\n**Core Functionalities:**\n- Host header validation to prevent DNS rebinding attacks\n- Cross-Origin Resource Sharing (CORS) protection with configurable origins",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_security.98fcde1d.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_serialization.2c029be6.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_serialization.2c029be6.lv1",
      "title": "test_serialization.2c029be6.lv1",
      "preview": "Assessment Source Management and Custom Scorer Framework Core Functionalities: Implement assessment source type validation and standardization for tracking evaluation origins (human, LLM judge, code-based) Create a flexible scorer framework supporting custom evaluation functions with serialization/deserialization capabilities Main Features & Requirements: Parse, validate, and standardize assessment source types with ",
      "preview_md": "## Task\n**Task Statement: Assessment Source Management and Custom Scorer Framework**\n\n**Core Functionalities:**\n- Implement assessment source type validation and standardization for tracking evaluation origins (human, LLM judge, code-based)",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_serialization.2c029be6.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_span.69efd376.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_span.69efd376.lv1",
      "title": "test_span.69efd376.lv1",
      "preview": "Implement MLflow Distributed Tracing System Core Functionalities: Create and manage distributed tracing spans with hierarchical relationships Support both live (mutable) and immutable span states during execution lifecycle Provide OpenTelemetry protocol compatibility for trace data exchange Enable trace data serialization/deserialization across different formats (dict, protobuf) Main Features & Requirements: Span lif",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Distributed Tracing System**\n\n**Core Functionalities:**\n- Create and manage distributed tracing spans with hierarchical relationships\n- Support both live (mutable) and immutable span states during execution lifecycle\n- Provide OpenTelemetry protocol compatibility for trace data exchange",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_span.69efd376.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_trace_correlation.53c200a6.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace_correlation.53c200a6.lv1",
      "title": "test_trace_correlation.53c200a6.lv1",
      "preview": "Statistical Correlation Analysis for Trace Data Implement functions to calculate Normalized Pointwise Mutual Information (NPMI) metrics for analyzing correlations between trace filters in machine learning experiments. The core functionality should: 1. Core Features: Calculate both unsmoothed and smoothed NPMI values from count statistics to measure association strength between two events/filters, normalized to a [-1,",
      "preview_md": "## Task\n**Task Statement: Statistical Correlation Analysis for Trace Data**\n\nImplement functions to calculate Normalized Pointwise Mutual Information (NPMI) metrics for analyzing correlations between trace filters in machine learning experiments. The core functionality should:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_trace_correlation.53c200a6.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_trace_correlation.53c200a6.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace_correlation.53c200a6.lv2",
      "title": "test_trace_correlation.53c200a6.lv2",
      "preview": "Statistical Correlation Analysis for Trace Data Implement functions to calculate Normalized Pointwise Mutual Information (NPMI) metrics for analyzing correlations between trace filters in machine learning experiments. The core functionality should: 1. Core Features: Calculate both unsmoothed and smoothed NPMI values from count statistics to measure association strength between two events/filters, normalized to a [-1,",
      "preview_md": "## Task\n**Task Statement: Statistical Correlation Analysis for Trace Data**\n\nImplement functions to calculate Normalized Pointwise Mutual Information (NPMI) metrics for analyzing correlations between trace filters in machine learning experiments. The core functionality should:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_trace_correlation.53c200a6.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_trace_manager.bb95fbcd.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace_manager.bb95fbcd.lv1",
      "title": "test_trace_manager.bb95fbcd.lv1",
      "preview": "Implement Distributed Tracing System Components Core Functionalities: Build a distributed tracing system that tracks execution flows across application components using spans and traces Manage span lifecycle including creation, attribute management, status tracking, and hierarchical relationships Provide in-memory trace management with thread-safe operations and automatic cleanup Main Features & Requirements: Impleme",
      "preview_md": "## Task\n**Task Statement: Implement Distributed Tracing System Components**\n\n**Core Functionalities:**\n- Build a distributed tracing system that tracks execution flows across application components using spans and traces\n- Manage span lifecycle including creation, attribute management, status tracking, and hierarchical relationships",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_trace_manager.bb95fbcd.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_trace_manager.bb95fbcd.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace_manager.bb95fbcd.lv2",
      "title": "test_trace_manager.bb95fbcd.lv2",
      "preview": "Implement Distributed Tracing System Components Core Functionalities: Build a distributed tracing system that tracks execution flows across application components using spans and traces Manage span lifecycle including creation, attribute management, status tracking, and hierarchical relationships Provide in-memory trace management with thread-safe operations and automatic cleanup Main Features & Requirements: Impleme",
      "preview_md": "## Task\n**Task Statement: Implement Distributed Tracing System Components**\n\n**Core Functionalities:**\n- Build a distributed tracing system that tracks execution flows across application components using spans and traces\n- Manage span lifecycle including creation, attribute management, status tracking, and hierarchical relationships",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_trace_manager.bb95fbcd.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_trace.17fde8b0.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace.17fde8b0.lv1",
      "title": "test_trace.17fde8b0.lv1",
      "preview": "MLflow Tracing System Implementation Core Functionalities: Implement a comprehensive distributed tracing system for MLflow that captures, manages, and analyzes execution flows across ML workflows and applications. Main Features & Requirements: Span Management: Create and manage hierarchical execution spans with lifecycle control, attributes, inputs/outputs, events, and status tracking Trace Data Handling: Store and r",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing System Implementation**\n\n**Core Functionalities:**\nImplement a comprehensive distributed tracing system for MLflow that captures, manages, and analyzes execution flows across ML workflows and applications.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_trace.17fde8b0.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_tracing.0dc1e3a3.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_tracing.0dc1e3a3.lv1",
      "title": "test_tracing.0dc1e3a3.lv1",
      "preview": "MLflow Tracing Integration for Conversational AI Systems Develop a tracing and logging system that captures, processes, and monitors conversational AI interactions by: 1. Core Functionalities: Parse and normalize timestamps from various formats to nanosecond precision Set up structured logging with configurable output destinations Process conversation transcripts to extract meaningful interaction data Generate standa",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing Integration for Conversational AI Systems**\n\nDevelop a tracing and logging system that captures, processes, and monitors conversational AI interactions by:\n\n1. **Core Functionalities:**\n   - Parse and normalize timestamps from various formats to nanosecond precision",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_tracing.0dc1e3a3.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_tracing.0dc1e3a3.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_tracing.0dc1e3a3.lv2",
      "title": "test_tracing.0dc1e3a3.lv2",
      "preview": "MLflow Tracing Integration for Conversational AI Systems Develop a tracing and logging system that captures, processes, and monitors conversational AI interactions by: 1. Core Functionalities: Parse and normalize timestamps from various formats to nanosecond precision Set up structured logging with configurable output destinations Process conversation transcripts to extract meaningful interaction data Generate standa",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing Integration for Conversational AI Systems**\n\nDevelop a tracing and logging system that captures, processes, and monitors conversational AI interactions by:\n\n1. **Core Functionalities:**\n   - Parse and normalize timestamps from various formats to nanosecond precision",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_tracing.0dc1e3a3.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_type_hints.12d7e575.lv2",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_type_hints.12d7e575.lv2",
      "title": "test_type_hints.12d7e575.lv2",
      "preview": "Data Type Inference and Validation System Develop a comprehensive data type inference and validation system that can: 1. Core Functionalities: Automatically infer data types and schemas from various input formats (pandas DataFrames, numpy arrays, lists, dictionaries, Pydantic models) Validate data against type hints and convert between compatible formats Handle complex nested data structures (arrays, objects, maps) a",
      "preview_md": "## Task\n**Task Statement: Data Type Inference and Validation System**\n\nDevelop a comprehensive data type inference and validation system that can:\n\n1. **Core Functionalities:**\n   - Automatically infer data types and schemas from various input formats (pandas DataFrames, numpy arrays, lists, dictionaries, Pydantic models)",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_type_hints.12d7e575.lv2/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_unity_catalog_models_artifact_repo.ad6e49d9.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_unity_catalog_models_artifact_repo.ad6e49d9.lv1",
      "title": "test_unity_catalog_models_artifact_repo.ad6e49d9.lv1",
      "preview": "Model Registry and Tracking Integration Implement functionality to retrieve model feature dependencies and logged model information from MLflow's tracking and Unity Catalog systems. The core objectives are: 1. Feature Dependency Extraction: Extract and format feature dependencies from model artifacts, handling both standard MLflow models and Databricks Feature Store models with appropriate validation and error handli",
      "preview_md": "## Task\n**Task Statement: Model Registry and Tracking Integration**\n\nImplement functionality to retrieve model feature dependencies and logged model information from MLflow's tracking and Unity Catalog systems. The core objectives are:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_unity_catalog_models_artifact_repo.ad6e49d9.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_unity_catalog_oss_rest_store.92593ea2.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_unity_catalog_oss_rest_store.92593ea2.lv1",
      "title": "test_unity_catalog_oss_rest_store.92593ea2.lv1",
      "preview": "Protocol Buffer to JSON Conversion with Type Preservation Develop utilities to convert Protocol Buffer messages to JSON format while preserving data type integrity, particularly for int64 fields that would otherwise be converted to strings. The solution must handle nested structures, maps, and repeated fields, then merge the type-corrected data back into the standard JSON representation to maintain numeric precision ",
      "preview_md": "## Task\n**Task Statement: Protocol Buffer to JSON Conversion with Type Preservation**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_unity_catalog_oss_rest_store.92593ea2.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_unity_catalog_rest_store.f47a7d9f.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_unity_catalog_rest_store.f47a7d9f.lv1",
      "title": "test_unity_catalog_rest_store.f47a7d9f.lv1",
      "preview": "Implement utility functions for protocol buffer and JSON data conversion, Unity Catalog tag transformation, dataset initialization, schema operations, and model signature type hint processing in an MLflow machine learning framework. Core Functionalities: Convert protocol buffer messages to JSON format while preserving int64 field types as numbers Transform MLflow tags to Unity Catalog-compatible tag formats for model",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement utility functions for protocol buffer and JSON data conversion, Unity Catalog tag transformation, dataset initialization, schema operations, and model signature type hint processing in an MLflow machine learning framework.\n\n**Core Functionalities:**",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_unity_catalog_rest_store.f47a7d9f.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_utils.3c9647bf.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_utils.3c9647bf.lv1",
      "title": "test_utils.3c9647bf.lv1",
      "preview": "OpenAI Chat Tool Configuration Parser Develop a tool parsing system that extracts and standardizes tool definitions from OpenAI chat API inputs. The system should: 1. Core Functionality: Parse various tool configuration formats (ChatCompletion API style, Responses API style, and built-in tools) into a unified ChatTool structure 2. Key Requirements: Handle multiple input formats and tool types Support both function-ba",
      "preview_md": "## Task\n**Task Statement: OpenAI Chat Tool Configuration Parser**\n\nDevelop a tool parsing system that extracts and standardizes tool definitions from OpenAI chat API inputs. The system should:",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_utils.3c9647bf.lv1/problem_statement.md"
    },
    {
      "id": "full:mlflow__mlflow.93dab383/test_validation.2e1c5076.lv1",
      "split": "full",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_validation.2e1c5076.lv1",
      "title": "test_validation.2e1c5076.lv1",
      "preview": "MLflow Data Validation System Implement a comprehensive validation system for MLflow experiment tracking that ensures data integrity and enforces business rules across machine learning workflows. Core Functionalities: Validate naming conventions and formats for metrics, parameters, tags, and model aliases Enforce length limits and character restrictions on various MLflow entities Check batch logging constraints and d",
      "preview_md": "## Task\n**Task Statement: MLflow Data Validation System**\n\nImplement a comprehensive validation system for MLflow experiment tracking that ensures data integrity and enforces business rules across machine learning workflows.\n\n**Core Functionalities:**\n- Validate naming conventions and formats for metrics, parameters, tags, and model aliases",
      "statement_path": "./data/full/mlflow__mlflow.93dab383/test_validation.2e1c5076.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_algorithms.1f0181c2.lv1",
      "title": "test_algorithms.1f0181c2.lv1",
      "preview": "Statistical Bootstrap Resampling Implementation Implement a bootstrap resampling algorithm that: 1. Core Functionality: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics 2. Key Features: Supports multiple resampling iterations with configurable sample sizes Handles various summary functions (mean, median, etc.) with automatic NaN-aware alter",
      "preview_md": "## Task\n**Task Statement: Statistical Bootstrap Resampling Implementation**\n\nImplement a bootstrap resampling algorithm that:\n\n1. **Core Functionality**: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics\n\n2. **Key Features**:",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv2",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_algorithms.1f0181c2.lv2",
      "title": "test_algorithms.1f0181c2.lv2",
      "preview": "Statistical Bootstrap Resampling Implementation Implement a bootstrap resampling algorithm that: 1. Core Functionality: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics 2. Key Features: Supports multiple resampling iterations with configurable sample sizes Handles various summary functions (mean, median, etc.) with automatic NaN-aware alter",
      "preview_md": "## Task\n**Task Statement: Statistical Bootstrap Resampling Implementation**\n\nImplement a bootstrap resampling algorithm that:\n\n1. **Core Functionality**: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics\n\n2. **Key Features**:",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv2/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_bar.123ed709.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_bar.123ed709.lv1",
      "title": "test_bar.123ed709.lv1",
      "preview": "Data Visualization Scale Transformation and Bar Chart Rendering Core Functionalities: Implement logarithmic scale transformations for data visualization axes Create and render bar chart visualizations with customizable properties Generate legend artists for bar chart elements Main Features & Requirements: Support mathematical transformations (logarithmic, power, etc.) for axis scaling Handle both individual bars and ",
      "preview_md": "## Task\n**Task Statement: Data Visualization Scale Transformation and Bar Chart Rendering**\n\n**Core Functionalities:**\n- Implement logarithmic scale transformations for data visualization axes\n- Create and render bar chart visualizations with customizable properties\n- Generate legend artists for bar chart elements",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_bar.123ed709.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_distributions.f700676d.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_distributions.f700676d.lv1",
      "title": "test_distributions.f700676d.lv1",
      "preview": "Color Palette and Data Visualization Interface Implementation Core Functionalities: Generate and manipulate color palettes (sequential, categorical, diverging) with support for various color spaces (RGB, HLS, HUSL, cubehelix) Create statistical data visualizations including distribution plots (histograms, KDE, ECDF) with semantic mappings Provide grid-based subplot layouts for multi-panel visualizations with faceting",
      "preview_md": "## Task\n**Task Statement: Color Palette and Data Visualization Interface Implementation**\n\n**Core Functionalities:**\n- Generate and manipulate color palettes (sequential, categorical, diverging) with support for various color spaces (RGB, HLS, HUSL, cubehelix)",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_distributions.f700676d.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_line.25304a8b.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_line.25304a8b.lv1",
      "title": "test_line.25304a8b.lv1",
      "preview": "Data Visualization Scale Normalization and Path Rendering System Implement a data visualization system that handles: 1. Scale Normalization: Create a normalization function for continuous data scales that transforms raw data values into a standardized range (0-1) based on specified minimum and maximum bounds, supporting both automatic data-driven bounds and user-defined limits. 2. Dodge Positioning: Develop positioni",
      "preview_md": "## Task\n**Task Statement: Data Visualization Scale Normalization and Path Rendering System**\n\nImplement a data visualization system that handles:",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_line.25304a8b.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_plot.b645d353.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_plot.b645d353.lv1",
      "title": "test_plot.b645d353.lv1",
      "preview": "Statistical Data Visualization Framework Implementation Implement a comprehensive statistical visualization framework that provides: Core Functionalities: Declarative plot specification with layered graphics (marks, statistics, transformations) Multi-dimensional data scaling and transformation (continuous, categorical, temporal) Flexible subplot layouts with faceting and variable pairing Automatic legend generation a",
      "preview_md": "## Task\n**Task Statement: Statistical Data Visualization Framework Implementation**\n\nImplement a comprehensive statistical visualization framework that provides:\n\n**Core Functionalities:**\n- Declarative plot specification with layered graphics (marks, statistics, transformations)",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_plot.b645d353.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_plot.b645d353.lv2",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_plot.b645d353.lv2",
      "title": "test_plot.b645d353.lv2",
      "preview": "Statistical Data Visualization Framework Implementation Implement a comprehensive statistical visualization framework that provides: Core Functionalities: Declarative plot specification with layered graphics (marks, statistics, transformations) Multi-dimensional data scaling and transformation (continuous, categorical, temporal) Flexible subplot layouts with faceting and variable pairing Automatic legend generation a",
      "preview_md": "## Task\n**Task Statement: Statistical Data Visualization Framework Implementation**\n\nImplement a comprehensive statistical visualization framework that provides:\n\n**Core Functionalities:**\n- Declarative plot specification with layered graphics (marks, statistics, transformations)",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_plot.b645d353.lv2/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_regression.ce8c62e2.lv1",
      "title": "test_regression.ce8c62e2.lv1",
      "preview": "Statistical Regression Plotting and Visualization Core Functionalities: Create scatter plots with fitted regression lines and confidence intervals Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed) Handle data preprocessing including variable extraction, missing data removal, and binning Generate both single plots and multi-faceted grid visualizations Main Features & Re",
      "preview_md": "## Task\n**Task Statement: Statistical Regression Plotting and Visualization**\n\n**Core Functionalities:**\n- Create scatter plots with fitted regression lines and confidence intervals\n- Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed)",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv2",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_regression.ce8c62e2.lv2",
      "title": "test_regression.ce8c62e2.lv2",
      "preview": "Statistical Regression Plotting and Visualization Core Functionalities: Create scatter plots with fitted regression lines and confidence intervals Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed) Handle data preprocessing including variable extraction, missing data removal, and binning Generate both single plots and multi-faceted grid visualizations Main Features & Re",
      "preview_md": "## Task\n**Task Statement: Statistical Regression Plotting and Visualization**\n\n**Core Functionalities:**\n- Create scatter plots with fitted regression lines and confidence intervals\n- Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed)",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv2/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_relational.f23eb542.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_relational.f23eb542.lv1",
      "title": "test_relational.f23eb542.lv1",
      "preview": "Statistical Data Visualization with Semantic Mapping Create a statistical plotting system that maps data variables to visual aesthetics (color, size, style) for relational plots. The system should: 1. Core Functionality: Generate scatter plots and line plots with automatic semantic mapping of categorical/numeric variables to visual properties like colors, marker sizes, and line styles. 2. Key Features: Support both f",
      "preview_md": "## Task\n**Task Statement: Statistical Data Visualization with Semantic Mapping**\n\nCreate a statistical plotting system that maps data variables to visual aesthetics (color, size, style) for relational plots. The system should:",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_relational.f23eb542.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_scales.d9dcdef6.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_scales.d9dcdef6.lv1",
      "title": "test_scales.d9dcdef6.lv1",
      "preview": "Color Palette Generation and Data Scaling System Implement a comprehensive system for generating color palettes and scaling data values for visualization purposes. The system should provide: Core Functionalities: Generate sequential, diverging, and categorical color palettes using various color space transformations (RGB, HUSL, cubehelix) Convert between different color spaces and formats (RGB, HUSL, LCH, XYZ, etc.) ",
      "preview_md": "## Task\n**Task Statement: Color Palette Generation and Data Scaling System**\n\nImplement a comprehensive system for generating color palettes and scaling data values for visualization purposes. The system should provide:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_scales.d9dcdef6.lv1/problem_statement.md"
    },
    {
      "id": "full:mwaskom__seaborn.7001ebe7/test_statistics.0f2ae277.lv1",
      "split": "full",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_statistics.0f2ae277.lv1",
      "title": "test_statistics.0f2ae277.lv1",
      "preview": "Statistical Data Preprocessing for Visualization Implement statistical transformation interfaces that prepare data for visualization by defining computational parameters and support structures. The core functionalities include: 1. KDE Support Definition: Create evaluation grids for kernel density estimation on univariate/bivariate data, handling bandwidth calculation, boundary clipping, and grid spacing. 2. Histogram",
      "preview_md": "## Task\n**Task Statement: Statistical Data Preprocessing for Visualization**\n\nImplement statistical transformation interfaces that prepare data for visualization by defining computational parameters and support structures. The core functionalities include:",
      "statement_path": "./data/full/mwaskom__seaborn.7001ebe7/test_statistics.0f2ae277.lv1/problem_statement.md"
    },
    {
      "id": "full:Netflix__metaflow.b390a8d4/test_stub_generator.7bf08c98.lv1",
      "split": "full",
      "repo_ref": "Netflix__metaflow.b390a8d4",
      "repo": "Netflix/metaflow",
      "case": "test_stub_generator.7bf08c98.lv1",
      "title": "test_stub_generator.7bf08c98.lv1",
      "preview": "Python Type Stub Generation for Dynamic Code Analysis Generate Python type stub (.pyi) files by dynamically analyzing runtime code structures. The system should: 1. Core Functionality: Introspect live Python modules to extract type information and generate corresponding stub files with proper type annotations, signatures, and documentation. 2. Key Features: Parse function signatures, class hierarchies, and type annot",
      "preview_md": "## Task\n**Task Statement: Python Type Stub Generation for Dynamic Code Analysis**\n\nGenerate Python type stub (.pyi) files by dynamically analyzing runtime code structures. The system should:",
      "statement_path": "./data/full/Netflix__metaflow.b390a8d4/test_stub_generator.7bf08c98.lv1/problem_statement.md"
    },
    {
      "id": "full:optuna__optuna.e7c6f1dd/test_heartbeat.5ad4d08f.lv1",
      "split": "full",
      "repo_ref": "optuna__optuna.e7c6f1dd",
      "repo": "optuna/optuna",
      "case": "test_heartbeat.5ad4d08f.lv1",
      "title": "test_heartbeat.5ad4d08f.lv1",
      "preview": "Experimental Feature Management and Trial Resilience System Develop a system that manages experimental features and provides trial resilience capabilities for optimization studies. The core functionalities include: 1. Experimental Feature Warnings: Implement warning mechanisms for experimental arguments and features to notify users about potentially unstable interfaces 2. Trial Health Monitoring: Create a heartbeat-b",
      "preview_md": "## Task\n**Task Statement: Experimental Feature Management and Trial Resilience System**\n\nDevelop a system that manages experimental features and provides trial resilience capabilities for optimization studies. The core functionalities include:",
      "statement_path": "./data/full/optuna__optuna.e7c6f1dd/test_heartbeat.5ad4d08f.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_all_methods.c74b49a1.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_all_methods.c74b49a1.lv1",
      "title": "test_all_methods.c74b49a1.lv1",
      "preview": "Implement GroupBy Statistical and Transformation Operations Core Functionalities: Implement statistical aggregation methods (correlation, statistical moments, extrema identification) and data transformation operations (cumulative operations, shifting, filling) for pandas GroupBy objects. Main Features & Requirements: Statistical aggregations: corrwith, skew, kurt, nunique for DataFrameGroupBy Index-based operations: ",
      "preview_md": "## Task\n**Task Statement: Implement GroupBy Statistical and Transformation Operations**\n\n**Core Functionalities:**\nImplement statistical aggregation methods (correlation, statistical moments, extrema identification) and data transformation operations (cumulative operations, shifting, filling) for pandas GroupBy objects.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_all_methods.c74b49a1.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_col.a592871d.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_col.a592871d.lv1",
      "title": "test_col.a592871d.lv1",
      "preview": "Implement a Deferred Column Expression System for DataFrame Operations Create a system that allows users to define column operations on DataFrames using symbolic expressions that are evaluated lazily. The system should: 1. Core Functionality: Provide a col() function to reference DataFrame columns symbolically and an Expression class to represent deferred computations that can be chained together before execution. 2.",
      "preview_md": "## Task\n**Task Statement: Implement a Deferred Column Expression System for DataFrame Operations**\n\nCreate a system that allows users to define column operations on DataFrames using symbolic expressions that are evaluated lazily. The system should:",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_col.a592871d.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_col.a592871d.lv2",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_col.a592871d.lv2",
      "title": "test_col.a592871d.lv2",
      "preview": "Implement a Deferred Column Expression System for DataFrame Operations Create a system that allows users to define column operations on DataFrames using symbolic expressions that are evaluated lazily. The system should: 1. Core Functionality: Provide a col() function to reference DataFrame columns symbolically and an Expression class to represent deferred computations that can be chained together before execution. 2.",
      "preview_md": "## Task\n**Task Statement: Implement a Deferred Column Expression System for DataFrame Operations**\n\nCreate a system that allows users to define column operations on DataFrames using symbolic expressions that are evaluated lazily. The system should:",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_col.a592871d.lv2/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_concat.ebe5de39.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_concat.ebe5de39.lv1",
      "title": "test_concat.ebe5de39.lv1",
      "preview": "Pandas Data Type Inference and Conversion System Core Functionalities: Implement a comprehensive data type inference, validation, and conversion system for pandas that handles diverse data structures including arrays, scalars, categorical data, sparse arrays, and datetime-like objects. Main Features & Requirements: Type Detection: Identify and classify various data types (dict-like, named tuples, numeric, datetime, c",
      "preview_md": "## Task\n**Task Statement: Pandas Data Type Inference and Conversion System**\n\n**Core Functionalities:**\nImplement a comprehensive data type inference, validation, and conversion system for pandas that handles diverse data structures including arrays, scalars, categorical data, sparse arrays, and datetime-like objects.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_concat.ebe5de39.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_describe.3b919815.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_describe.3b919815.lv1",
      "title": "test_describe.3b919815.lv1",
      "preview": "Pandas Data Structure Validation and Type Conversion Develop a code agent to handle core pandas data structure operations including: 1. Type System Management: Handle extension data types, storage backends, and type conversions between pandas and PyArrow formats 2. Data Validation: Validate indexing operations, categorical data integrity, boolean indexers, and parameter constraints 3. Index Operations: Perform set op",
      "preview_md": "## Task\n**Task Statement: Pandas Data Structure Validation and Type Conversion**\n\nDevelop a code agent to handle core pandas data structure operations including:\n\n1. **Type System Management**: Handle extension data types, storage backends, and type conversions between pandas and PyArrow formats",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_describe.3b919815.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_formats.05080a4e.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_formats.05080a4e.lv1",
      "title": "test_formats.05080a4e.lv1",
      "preview": "Implement extension data type infrastructure for pandas that supports custom array types with proper NA value handling and datetime-compatible assignment operations. Core Functionalities: Define extensible data type system allowing third-party custom dtypes to integrate with pandas Provide default NA value management for extension types through the na_value property Enable safe assignment operations for datetime-like",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement extension data type infrastructure for pandas that supports custom array types with proper NA value handling and datetime-compatible assignment operations.\n\n**Core Functionalities:**\n- Define extensible data type system allowing third-party custom dtypes to integrate with pandas",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_formats.05080a4e.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_groupby_shift_diff.e13d5358.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_groupby_shift_diff.e13d5358.lv1",
      "title": "test_groupby_shift_diff.e13d5358.lv1",
      "preview": "Pandas GroupBy Data Processing and Formatting System Implement a comprehensive data processing system that provides: 1. Core Functionalities: Group-based data aggregation, transformation, and filtering operations Efficient data splitting and iteration over grouped datasets Flexible object formatting and display with customizable output styles Internal data structure management for columnar data storage 2. Main Featur",
      "preview_md": "## Task\n**Task Statement: Pandas GroupBy Data Processing and Formatting System**\n\nImplement a comprehensive data processing system that provides:\n\n1. **Core Functionalities:**\n   - Group-based data aggregation, transformation, and filtering operations\n   - Efficient data splitting and iteration over grouped datasets",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_groupby_shift_diff.e13d5358.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_http_headers.aafb551e.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_http_headers.aafb551e.lv1",
      "title": "test_http_headers.aafb551e.lv1",
      "preview": "Data I/O Interface Implementation Task Implement a comprehensive data input/output system that provides: 1. Core Functionalities: Read data from multiple file formats (CSV, JSON, HTML, Parquet, Pickle, Stata) Format and render data for display (HTML output, engineering notation) Handle various data serialization and deserialization operations 2. Main Features & Requirements: Support multiple parsing engines and backe",
      "preview_md": "## Task\n**Data I/O Interface Implementation Task**\n\nImplement a comprehensive data input/output system that provides:\n\n1. **Core Functionalities:**\n   - Read data from multiple file formats (CSV, JSON, HTML, Parquet, Pickle, Stata)\n   - Format and render data for display (HTML output, engineering notation)",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_http_headers.aafb551e.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_iceberg.85771c70.lv1",
      "title": "test_iceberg.85771c70.lv1",
      "preview": "Apache Iceberg Table Integration Implement functionality to read Apache Iceberg tables into pandas DataFrames with support for: Core Functionalities: Connect to Iceberg catalogs and load table data Convert Iceberg table format to pandas DataFrame format Support flexible data filtering and column selection Main Features & Requirements: Table identification and catalog configuration management Row filtering with custom",
      "preview_md": "## Task\n**Task Statement: Apache Iceberg Table Integration**\n\nImplement functionality to read Apache Iceberg tables into pandas DataFrames with support for:\n\n**Core Functionalities:**\n- Connect to Iceberg catalogs and load table data\n- Convert Iceberg table format to pandas DataFrame format",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv2",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_iceberg.85771c70.lv2",
      "title": "test_iceberg.85771c70.lv2",
      "preview": "Apache Iceberg Table Integration Implement functionality to read Apache Iceberg tables into pandas DataFrames with support for: Core Functionalities: Connect to Iceberg catalogs and load table data Convert Iceberg table format to pandas DataFrame format Support flexible data filtering and column selection Main Features & Requirements: Table identification and catalog configuration management Row filtering with custom",
      "preview_md": "## Task\n**Task Statement: Apache Iceberg Table Integration**\n\nImplement functionality to read Apache Iceberg tables into pandas DataFrames with support for:\n\n**Core Functionalities:**\n- Connect to Iceberg catalogs and load table data\n- Convert Iceberg table format to pandas DataFrame format",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv2/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_info.d8a64ebf.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_info.d8a64ebf.lv1",
      "title": "test_info.d8a64ebf.lv1",
      "preview": "Implement GroupBy Data Splitting and Grouping Operations Core Functionalities: Create grouping mechanisms that partition pandas DataFrames/Series into logical groups based on specified keys, levels, or custom groupers Implement data splitting operations that efficiently divide datasets while preserving index relationships and handling categorical data Support multiple grouping strategies including column-based, index",
      "preview_md": "## Task\n**Task Statement: Implement GroupBy Data Splitting and Grouping Operations**\n\n**Core Functionalities:**\n- Create grouping mechanisms that partition pandas DataFrames/Series into logical groups based on specified keys, levels, or custom groupers",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_info.d8a64ebf.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_list_accessor.7ab0b2ea.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_list_accessor.7ab0b2ea.lv1",
      "title": "test_list_accessor.7ab0b2ea.lv1",
      "preview": "Implement Arrow-backed data accessors and type conversion utilities for pandas that provide: 1. Core functionalities: Specialized accessor classes for Arrow list and struct data types that enable element-wise operations, validation, and data manipulation on PyArrow-backed pandas Series. 2. Main features and requirements: List accessor supporting indexing, slicing, length calculation, flattening, and iteration control",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement Arrow-backed data accessors and type conversion utilities for pandas that provide:\n\n1. **Core functionalities**: Specialized accessor classes for Arrow list and struct data types that enable element-wise operations, validation, and data manipulation on PyArrow-backed pandas Series.",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_list_accessor.7ab0b2ea.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_merge_antijoin.921feefe.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_merge_antijoin.921feefe.lv1",
      "title": "test_merge_antijoin.921feefe.lv1",
      "preview": "Data Transformation and Merging Operations Implement core data manipulation functionalities for pandas-like operations including: 1. Safe Type Conversion: Convert array data types while handling NaN values, extension arrays, and maintaining data integrity across different numeric and categorical types. 2. Data Concatenation: Combine multiple data structures (BlockManagers) along specified axes, handling column alignm",
      "preview_md": "## Task\n**Task Statement: Data Transformation and Merging Operations**\n\nImplement core data manipulation functionalities for pandas-like operations including:\n\n1. **Safe Type Conversion**: Convert array data types while handling NaN values, extension arrays, and maintaining data integrity across different numeric and categorical types.",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_merge_antijoin.921feefe.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_nlargest_nsmallest.08e879a9.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_nlargest_nsmallest.08e879a9.lv1",
      "title": "test_nlargest_nsmallest.08e879a9.lv1",
      "preview": "Implement grouped data selection methods that return the N largest or smallest values within each group of a pandas Series. The methods should support configurable result size, handle duplicate value tie-breaking strategies, and maintain proper indexing while preserving group structure in the output. Key considerations include efficient handling of large datasets, proper NA value treatment, and ensuring consistent be",
      "preview_md": "## Task\n**Task Statement:**",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_nlargest_nsmallest.08e879a9.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_quantile.f4b69e49.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_quantile.f4b69e49.lv1",
      "title": "test_quantile.f4b69e49.lv1",
      "preview": "Pandas Data Type Management and Casting Operations Core Functionalities: Implement data type introspection, validation, and casting operations for pandas extension types and datetime-like objects, focusing on safe type conversions and metadata handling. Main Features & Requirements: Provide field name enumeration for structured extension data types Convert various input formats (arrays, scalars) to standardized datet",
      "preview_md": "## Task\n**Task Statement: Pandas Data Type Management and Casting Operations**\n\n**Core Functionalities:**\nImplement data type introspection, validation, and casting operations for pandas extension types and datetime-like objects, focusing on safe type conversions and metadata handling.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_quantile.f4b69e49.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_reduction.3ac3b298.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_reduction.3ac3b298.lv1",
      "title": "test_reduction.3ac3b298.lv1",
      "preview": "Implement data manipulation and aggregation operations for grouped data structures, focusing on: 1. Core Functionalities: Array element repetition and duplication Statistical computations (sum, product, min, max, variance, standard deviation, skewness) on grouped data Boolean aggregations (any, all) across group elements 2. Main Features & Requirements: Handle missing values (NA/NaN) appropriately with skipna paramet",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement data manipulation and aggregation operations for grouped data structures, focusing on:\n\n1. **Core Functionalities:**\n   - Array element repetition and duplication\n   - Statistical computations (sum, product, min, max, variance, standard deviation, skewness) on grouped data",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_reduction.3ac3b298.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_sample.5d2ba03e.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_sample.5d2ba03e.lv1",
      "title": "test_sample.5d2ba03e.lv1",
      "preview": "Random State Standardization and Validation Implement a utility function that standardizes and validates random state inputs for reproducible random number generation. The function should: 1. Core Functionality: Accept various random state formats (integers, arrays, generators, etc.) and convert them to appropriate NumPy random state objects 2. Key Requirements: Handle multiple input types (int, array-like, BitGenera",
      "preview_md": "## Task\n**Task Statement: Random State Standardization and Validation**\n\nImplement a utility function that standardizes and validates random state inputs for reproducible random number generation. The function should:",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_sample.5d2ba03e.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_spec_conformance.3aff206b.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_spec_conformance.3aff206b.lv1",
      "title": "test_spec_conformance.3aff206b.lv1",
      "preview": "Pandas DataFrame Interchange Protocol Implementation Implement a standardized data interchange interface that enables seamless data exchange between pandas DataFrames and other data processing libraries. The system should provide: Core Functionalities: DataFrame-level operations: access metadata, dimensions, column selection, and chunking capabilities Column-level operations: data type inspection, null value handling",
      "preview_md": "## Task\n**Task Statement: Pandas DataFrame Interchange Protocol Implementation**\n\nImplement a standardized data interchange interface that enables seamless data exchange between pandas DataFrames and other data processing libraries. The system should provide:\n\n**Core Functionalities:**",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_spec_conformance.3aff206b.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_string.dcfa24ea.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_string.dcfa24ea.lv1",
      "title": "test_string.dcfa24ea.lv1",
      "preview": "Implement Pandas Data Formatting and Display System Core Functionalities: 1. Data Type System: Implement extension data types with boolean/numeric classification, missing value handling, and storage backend abstraction 2. Array Operations: Provide membership testing (isin), iteration protocols, shape/dimension properties, and string representation for extension arrays 3. Text Formatting: Create adaptive text adjustme",
      "preview_md": "## Task\n**Task Statement: Implement Pandas Data Formatting and Display System**\n\n**Core Functionalities:**\n1. **Data Type System**: Implement extension data types with boolean/numeric classification, missing value handling, and storage backend abstraction",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_string.dcfa24ea.lv1/problem_statement.md"
    },
    {
      "id": "full:pandas-dev__pandas.82fa2715/test_struct_accessor.3b465152.lv1",
      "split": "full",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_struct_accessor.3b465152.lv1",
      "title": "test_struct_accessor.3b465152.lv1",
      "preview": "Implement PyArrow-backed data structure accessors and type conversion utilities for pandas, focusing on: 1. Core Functionalities: Provide specialized accessors for structured data types (struct fields) in PyArrow arrays Enable seamless conversion between pandas dtypes and PyArrow data types Support field extraction, type introspection, and data explosion operations 2. Main Features & Requirements: Struct field access",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement PyArrow-backed data structure accessors and type conversion utilities for pandas, focusing on:\n\n1. **Core Functionalities:**\n   - Provide specialized accessors for structured data types (struct fields) in PyArrow arrays\n   - Enable seamless conversion between pandas dtypes and PyArrow data types",
      "statement_path": "./data/full/pandas-dev__pandas.82fa2715/test_struct_accessor.3b465152.lv1/problem_statement.md"
    },
    {
      "id": "full:pydantic__pydantic.e1dcaf9e/test_deprecated_fields.40a2ec54.lv1",
      "split": "full",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_deprecated_fields.40a2ec54.lv1",
      "title": "test_deprecated_fields.40a2ec54.lv1",
      "preview": "Implement Property and Field Management System Create a system that handles property descriptors, field metadata, and computed fields for data validation models. The core functionalities include: 1. Property Descriptor Management: Convert functions to properties, handle deprecated field access with runtime warnings, and manage private attribute detection 2. Field Metadata Processing: Collect and merge field informati",
      "preview_md": "## Task\n**Task Statement: Implement Property and Field Management System**\n\nCreate a system that handles property descriptors, field metadata, and computed fields for data validation models. The core functionalities include:",
      "statement_path": "./data/full/pydantic__pydantic.e1dcaf9e/test_deprecated_fields.40a2ec54.lv1/problem_statement.md"
    },
    {
      "id": "full:pydantic__pydantic.e1dcaf9e/test_experimental_arguments_schema.00dc2dd4.lv1",
      "split": "full",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_experimental_arguments_schema.00dc2dd4.lv1",
      "title": "test_experimental_arguments_schema.00dc2dd4.lv1",
      "preview": "Function Argument Schema Generation Generate validation schemas for callable function arguments by analyzing function signatures and type annotations. The system should dynamically inspect function parameters, handle different schema formats, support parameter filtering through callbacks, and produce core schemas compatible with validation frameworks. Key considerations include proper type annotation processing, name",
      "preview_md": "## Task\n**Task Statement: Function Argument Schema Generation**",
      "statement_path": "./data/full/pydantic__pydantic.e1dcaf9e/test_experimental_arguments_schema.00dc2dd4.lv1/problem_statement.md"
    },
    {
      "id": "full:pydantic__pydantic.e1dcaf9e/test_pipeline.c9b08962.lv1",
      "split": "full",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_pipeline.c9b08962.lv1",
      "title": "test_pipeline.c9b08962.lv1",
      "preview": "Data Validation and Transformation Pipeline System Build a flexible data validation and transformation system that provides: Core Functionalities: Type adaptation and schema generation for arbitrary Python types Chainable validation pipelines with transformation steps JSON schema generation and serialization capabilities Constraint application (comparisons, string operations, datetime handling) Main Features: Support",
      "preview_md": "## Task\n**Task Statement: Data Validation and Transformation Pipeline System**\n\nBuild a flexible data validation and transformation system that provides:\n\n**Core Functionalities:**\n- Type adaptation and schema generation for arbitrary Python types\n- Chainable validation pipelines with transformation steps",
      "statement_path": "./data/full/pydantic__pydantic.e1dcaf9e/test_pipeline.c9b08962.lv1/problem_statement.md"
    },
    {
      "id": "full:pydantic__pydantic.e1dcaf9e/test_titles.e806bda8.lv1",
      "split": "full",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_titles.e806bda8.lv1",
      "title": "test_titles.e806bda8.lv1",
      "preview": "Task: Implement Computed Field Property Management System Create a system for managing computed fields in data models that supports: 1. Property Conversion & Validation: Convert functions to properties, validate property types, and ensure proper descriptor behavior 2. Field Metadata Management: Handle computed field configuration including aliases, titles, descriptions, deprecation warnings, and JSON schema extras 3.",
      "preview_md": "## Task\n**Task: Implement Computed Field Property Management System**\n\nCreate a system for managing computed fields in data models that supports:\n\n1. **Property Conversion & Validation**: Convert functions to properties, validate property types, and ensure proper descriptor behavior",
      "statement_path": "./data/full/pydantic__pydantic.e1dcaf9e/test_titles.e806bda8.lv1/problem_statement.md"
    },
    {
      "id": "full:pydantic__pydantic.e1dcaf9e/test_types_self.3ae7e16d.lv1",
      "split": "full",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_types_self.3ae7e16d.lv1",
      "title": "test_types_self.3ae7e16d.lv1",
      "preview": "Task: Implement Pydantic Data Validation and Serialization System Create a comprehensive data validation and serialization framework that provides: Core Functionalities: Model-based data validation with automatic type checking and conversion Flexible serialization/deserialization between Python objects, JSON, and dictionaries Runtime validation of function calls and their arguments Type adapter system for validating ",
      "preview_md": "## Task\n**Task: Implement Pydantic Data Validation and Serialization System**\n\nCreate a comprehensive data validation and serialization framework that provides:\n\n**Core Functionalities:**\n- Model-based data validation with automatic type checking and conversion\n- Flexible serialization/deserialization between Python objects, JSON, and dictionaries",
      "statement_path": "./data/full/pydantic__pydantic.e1dcaf9e/test_types_self.3ae7e16d.lv1/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_backends_chunks.fa55f68a.lv1",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_backends_chunks.fa55f68a.lv1",
      "title": "test_backends_chunks.fa55f68a.lv1",
      "preview": "Multi-dimensional Array Chunk Alignment and Grid-based Rechunking Core Functionalities: Align chunked array data structures with backend storage grid patterns Generate uniform grid-based chunk layouts for multi-dimensional arrays Rechunk variables to optimize memory usage and storage alignment Main Features & Requirements: Handle multi-dimensional arrays with varying chunk sizes across dimensions Preserve data integr",
      "preview_md": "## Task\n**Task Statement: Multi-dimensional Array Chunk Alignment and Grid-based Rechunking**\n\n**Core Functionalities:**\n- Align chunked array data structures with backend storage grid patterns\n- Generate uniform grid-based chunk layouts for multi-dimensional arrays\n- Rechunk variables to optimize memory usage and storage alignment",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_backends_chunks.fa55f68a.lv1/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_coordinate_transform.6cacb660.lv1",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_coordinate_transform.6cacb660.lv1",
      "title": "test_coordinate_transform.6cacb660.lv1",
      "preview": "Xarray Coordinate and Index Management System Implement a comprehensive coordinate and index management system for xarray that provides: Core Functionalities: Coordinate transformation and mapping between grid/world coordinate systems Multi-dimensional array indexing and selection operations Coordinate merging, alignment, and validation across datasets Index creation, manipulation, and compatibility checking Main Fea",
      "preview_md": "## Task\n**Task Statement: Xarray Coordinate and Index Management System**\n\nImplement a comprehensive coordinate and index management system for xarray that provides:\n\n**Core Functionalities:**\n- Coordinate transformation and mapping between grid/world coordinate systems\n- Multi-dimensional array indexing and selection operations",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_coordinate_transform.6cacb660.lv1/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_coordinate_transform.6cacb660.lv2",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_coordinate_transform.6cacb660.lv2",
      "title": "test_coordinate_transform.6cacb660.lv2",
      "preview": "Xarray Coordinate and Index Management System Implement a comprehensive coordinate and index management system for xarray that provides: Core Functionalities: Coordinate transformation and mapping between grid/world coordinate systems Multi-dimensional array indexing and selection operations Coordinate merging, alignment, and validation across datasets Index creation, manipulation, and compatibility checking Main Fea",
      "preview_md": "## Task\n**Task Statement: Xarray Coordinate and Index Management System**\n\nImplement a comprehensive coordinate and index management system for xarray that provides:\n\n**Core Functionalities:**\n- Coordinate transformation and mapping between grid/world coordinate systems\n- Multi-dimensional array indexing and selection operations",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_coordinate_transform.6cacb660.lv2/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_range_index.7b95ba66.lv1",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_range_index.7b95ba66.lv1",
      "title": "test_range_index.7b95ba66.lv1",
      "preview": "Implement array data type conversion and validation utilities for xarray's coordinate and variable system. The core functionalities include: 1. Data Type Conversion: Convert arrays between different formats (numpy, pandas, extension arrays) while preserving metadata and handling edge cases like datetime types and missing values. 2. Array Validation: Validate array compatibility for extension arrays, numpy dtypes, and",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement array data type conversion and validation utilities for xarray's coordinate and variable system. The core functionalities include:",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_range_index.7b95ba66.lv1/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_treenode.aa8ba777.lv1",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_treenode.aa8ba777.lv1",
      "title": "test_treenode.aa8ba777.lv1",
      "preview": "Tree Data Structure Implementation Implement a hierarchical tree node system that supports: 1. Core Functionalities: Parent-child relationships with bidirectional navigation Named and unnamed node variants Tree traversal operations (ancestors, descendants, siblings) Path-based node access using Unix-like syntax 2. Main Features: Node lifecycle management (attach, detach, orphan operations) Tree structure validation a",
      "preview_md": "## Task\n**Task Statement: Tree Data Structure Implementation**\n\nImplement a hierarchical tree node system that supports:\n\n1. **Core Functionalities:**\n   - Parent-child relationships with bidirectional navigation\n   - Named and unnamed node variants",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_treenode.aa8ba777.lv1/problem_statement.md"
    },
    {
      "id": "full:pydata__xarray.97f3a746/test_treenode.aa8ba777.lv2",
      "split": "full",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_treenode.aa8ba777.lv2",
      "title": "test_treenode.aa8ba777.lv2",
      "preview": "Tree Data Structure Implementation Implement a hierarchical tree node system that supports: 1. Core Functionalities: Parent-child relationships with bidirectional navigation Named and unnamed node variants Tree traversal operations (ancestors, descendants, siblings) Path-based node access using Unix-like syntax 2. Main Features: Node lifecycle management (attach, detach, orphan operations) Tree structure validation a",
      "preview_md": "## Task\n**Task Statement: Tree Data Structure Implementation**\n\nImplement a hierarchical tree node system that supports:\n\n1. **Core Functionalities:**\n   - Parent-child relationships with bidirectional navigation\n   - Named and unnamed node variants",
      "statement_path": "./data/full/pydata__xarray.97f3a746/test_treenode.aa8ba777.lv2/problem_statement.md"
    },
    {
      "id": "full:pypa__hatch.ff4b4040/test_config.38d2ef18.lv1",
      "split": "full",
      "repo_ref": "pypa__hatch.ff4b4040",
      "repo": "pypa/hatch",
      "case": "test_config.38d2ef18.lv1",
      "title": "test_config.38d2ef18.lv1",
      "preview": "Python Project Configuration Management Implement a configuration system for Python project management that handles: Core Functionalities: Environment dependency management and requirement specification Build configuration with target-specific settings and dependencies Publishing configuration for package distribution Build hook system with conditional enablement Main Features: Parse and validate project configuratio",
      "preview_md": "## Task\n**Task Statement: Python Project Configuration Management**\n\nImplement a configuration system for Python project management that handles:\n\n**Core Functionalities:**\n- Environment dependency management and requirement specification\n- Build configuration with target-specific settings and dependencies",
      "statement_path": "./data/full/pypa__hatch.ff4b4040/test_config.38d2ef18.lv1/problem_statement.md"
    },
    {
      "id": "full:pypa__hatch.ff4b4040/test_fmt.782c88a8.lv1",
      "split": "full",
      "repo_ref": "pypa__hatch.ff4b4040",
      "repo": "pypa/hatch",
      "case": "test_fmt.782c88a8.lv1",
      "title": "test_fmt.782c88a8.lv1",
      "preview": "Implement a Python project management and environment system that provides: 1. Core functionalities: Cross-platform command execution and process management Project configuration parsing and metadata handling Virtual environment creation, management, and dependency synchronization Build system integration and plugin architecture support 2. Main features and requirements: Platform-agnostic shell command formatting and",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement a Python project management and environment system that provides:\n\n1. **Core functionalities:**\n   - Cross-platform command execution and process management\n   - Project configuration parsing and metadata handling",
      "statement_path": "./data/full/pypa__hatch.ff4b4040/test_fmt.782c88a8.lv1/problem_statement.md"
    },
    {
      "id": "full:pypa__packaging.013f3b03/test_metadata.e00b5801.lv1",
      "split": "full",
      "repo_ref": "pypa__packaging.013f3b03",
      "repo": "pypa/packaging",
      "case": "test_metadata.e00b5801.lv1",
      "title": "test_metadata.e00b5801.lv1",
      "preview": "Python Package Metadata Processing and Validation Core Functionalities: Parse and validate Python package metadata from various formats (email headers, raw dictionaries) Convert between different metadata representations and enforce metadata specification compliance Handle requirement strings parsing and validation according to PEP 508 Main Features & Requirements: Support multiple metadata versions (1.0-2.5) with ve",
      "preview_md": "## Task\n**Task Statement: Python Package Metadata Processing and Validation**\n\n**Core Functionalities:**\n- Parse and validate Python package metadata from various formats (email headers, raw dictionaries)\n- Convert between different metadata representations and enforce metadata specification compliance",
      "statement_path": "./data/full/pypa__packaging.013f3b03/test_metadata.e00b5801.lv1/problem_statement.md"
    },
    {
      "id": "full:pypa__setuptools.d198e86f/test_bdist_wheel.51482fc6.lv1",
      "split": "full",
      "repo_ref": "pypa__setuptools.d198e86f",
      "repo": "pypa/setuptools",
      "case": "test_bdist_wheel.51482fc6.lv1",
      "title": "test_bdist_wheel.51482fc6.lv1",
      "preview": "Python Package Build and Installation Command Interface Implementation Core Functionalities: Implement command interfaces for Python package building, compilation, and installation processes, including configuration management, extension building, and file deployment. Main Features & Requirements: Parse and apply project configuration from pyproject.toml files with dynamic field resolution Generate platform-specific ",
      "preview_md": "## Task\n**Task Statement: Python Package Build and Installation Command Interface Implementation**\n\n**Core Functionalities:**\nImplement command interfaces for Python package building, compilation, and installation processes, including configuration management, extension building, and file deployment.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/full/pypa__setuptools.d198e86f/test_bdist_wheel.51482fc6.lv1/problem_statement.md"
    },
    {
      "id": "full:pytest-dev__pytest.68016f0e/raises_group.c28bf36a.lv1",
      "split": "full",
      "repo_ref": "pytest-dev__pytest.68016f0e",
      "repo": "pytest-dev/pytest",
      "case": "raises_group.c28bf36a.lv1",
      "title": "raises_group.c28bf36a.lv1",
      "preview": "Exception Group Testing Framework Develop a comprehensive exception testing system that handles both individual exceptions and nested exception groups with the following core functionalities: 1. Exception Group Parsing and Validation: Parse and validate complex nested exception structures, including BaseExceptionGroup and ExceptionGroup hierarchies with support for flattening subgroups and unwrapping single exception",
      "preview_md": "## Task\n**Task Statement: Exception Group Testing Framework**\n\nDevelop a comprehensive exception testing system that handles both individual exceptions and nested exception groups with the following core functionalities:",
      "statement_path": "./data/full/pytest-dev__pytest.68016f0e/raises_group.c28bf36a.lv1/problem_statement.md"
    },
    {
      "id": "full:pytest-dev__pytest.68016f0e/test_local.40fb2f1f.lv1",
      "split": "full",
      "repo_ref": "pytest-dev__pytest.68016f0e",
      "repo": "pytest-dev/pytest",
      "case": "test_local.40fb2f1f.lv1",
      "title": "test_local.40fb2f1f.lv1",
      "preview": "File System Path Manipulation and Error Handling Library Implement a comprehensive file system abstraction layer that provides: 1. Core Functionalities: Object-oriented path manipulation (join, navigate, compare paths) File/directory operations (create, read, write, copy, move, delete) Cross-platform compatibility with proper path separators and encoding Dynamic error class generation based on system errno codes 2. M",
      "preview_md": "## Task\n**Task Statement: File System Path Manipulation and Error Handling Library**\n\nImplement a comprehensive file system abstraction layer that provides:\n\n1. **Core Functionalities:**\n   - Object-oriented path manipulation (join, navigate, compare paths)\n   - File/directory operations (create, read, write, copy, move, delete)",
      "statement_path": "./data/full/pytest-dev__pytest.68016f0e/test_local.40fb2f1f.lv1/problem_statement.md"
    },
    {
      "id": "full:python__mypy.8e2ce962/testconstraints.db380fe7.lv1",
      "split": "full",
      "repo_ref": "python__mypy.8e2ce962",
      "repo": "python/mypy",
      "case": "testconstraints.db380fe7.lv1",
      "title": "testconstraints.db380fe7.lv1",
      "preview": "Type Constraint Equality and Hashing Implement equality comparison and hash functionality for type inference constraints in a static type checker. The core functionality involves: 1. Constraint Representation: Handle constraints that represent relationships between type variables and target types (subtype/supertype relationships) 2. Equality Semantics: Define when two constraints are considered equal based on their t",
      "preview_md": "## Task\n**Task Statement: Type Constraint Equality and Hashing**\n\nImplement equality comparison and hash functionality for type inference constraints in a static type checker. The core functionality involves:",
      "statement_path": "./data/full/python__mypy.8e2ce962/testconstraints.db380fe7.lv1/problem_statement.md"
    },
    {
      "id": "full:python__mypy.8e2ce962/testconstraints.db380fe7.lv2",
      "split": "full",
      "repo_ref": "python__mypy.8e2ce962",
      "repo": "python/mypy",
      "case": "testconstraints.db380fe7.lv2",
      "title": "testconstraints.db380fe7.lv2",
      "preview": "Type Constraint Equality and Hashing Implement equality comparison and hash functionality for type inference constraints in a static type checker. The core functionality involves: 1. Constraint Representation: Handle constraints that represent relationships between type variables and target types (subtype/supertype relationships) 2. Equality Semantics: Define when two constraints are considered equal based on their t",
      "preview_md": "## Task\n**Task Statement: Type Constraint Equality and Hashing**\n\nImplement equality comparison and hash functionality for type inference constraints in a static type checker. The core functionality involves:",
      "statement_path": "./data/full/python__mypy.8e2ce962/testconstraints.db380fe7.lv2/problem_statement.md"
    },
    {
      "id": "full:scikit-learn__scikit-learn.5741bac9/test_arff_parser.ecde431a.lv1",
      "split": "full",
      "repo_ref": "scikit-learn__scikit-learn.5741bac9",
      "repo": "scikit-learn/scikit-learn",
      "case": "test_arff_parser.ecde431a.lv1",
      "title": "test_arff_parser.ecde431a.lv1",
      "preview": "ARFF Dataset Processing and Column Selection Develop functionality to parse and process ARFF (Attribute-Relation File Format) datasets from compressed files, with the ability to selectively extract features and targets into different output formats (NumPy arrays, sparse matrices, or pandas DataFrames). Core Functionalities: Load and parse compressed ARFF files using multiple parsing strategies Split datasets into fea",
      "preview_md": "## Task\n**Task Statement: ARFF Dataset Processing and Column Selection**\n\nDevelop functionality to parse and process ARFF (Attribute-Relation File Format) datasets from compressed files, with the ability to selectively extract features and targets into different output formats (NumPy arrays, sparse matrices, or pandas DataFrames).\n\n**Core Functionalities:**",
      "statement_path": "./data/full/scikit-learn__scikit-learn.5741bac9/test_arff_parser.ecde431a.lv1/problem_statement.md"
    },
    {
      "id": "full:scikit-learn__scikit-learn.5741bac9/test_predict_error_display.11cc0c3a.lv1",
      "split": "full",
      "repo_ref": "scikit-learn__scikit-learn.5741bac9",
      "repo": "scikit-learn/scikit-learn",
      "case": "test_predict_error_display.11cc0c3a.lv1",
      "title": "test_predict_error_display.11cc0c3a.lv1",
      "preview": "Regression Model Prediction Error Visualization Create visualization tools for assessing regression model performance through scatter plots that display prediction errors. The interfaces should support two visualization modes: actual vs predicted values and residuals vs predicted values. Key requirements include handling both fitted estimators and direct prediction inputs, supporting data subsampling for large datase",
      "preview_md": "## Task\n**Task Statement: Regression Model Prediction Error Visualization**",
      "statement_path": "./data/full/scikit-learn__scikit-learn.5741bac9/test_predict_error_display.11cc0c3a.lv1/problem_statement.md"
    },
    {
      "id": "full:scikit-learn__scikit-learn.5741bac9/test_public_functions.28421aef.lv1",
      "split": "full",
      "repo_ref": "scikit-learn__scikit-learn.5741bac9",
      "repo": "scikit-learn/scikit-learn",
      "case": "test_public_functions.28421aef.lv1",
      "title": "test_public_functions.28421aef.lv1",
      "preview": "Implement Machine Learning Parameter Validation and Data Preprocessing Functions Core Functionalities: Generate valid/invalid parameter values for testing ML algorithm constraints Apply various data scaling and transformation techniques (min-max, robust, quantile, power transforms) Perform unsupervised learning algorithms including clustering (affinity propagation, DBSCAN, mean shift, spectral) and matrix decompositi",
      "preview_md": "## Task\n**Task Statement: Implement Machine Learning Parameter Validation and Data Preprocessing Functions**\n\n**Core Functionalities:**\n- Generate valid/invalid parameter values for testing ML algorithm constraints\n- Apply various data scaling and transformation techniques (min-max, robust, quantile, power transforms)",
      "statement_path": "./data/full/scikit-learn__scikit-learn.5741bac9/test_public_functions.28421aef.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_build_gettext.2721e644.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_build_gettext.2721e644.lv1",
      "title": "test_build_gettext.2721e644.lv1",
      "preview": "Internationalization Template and Message Catalog System Implement a system for extracting, organizing, and rendering translatable messages from documentation sources. The core functionality should: 1. Message Catalog Management: Create a catalog system that can collect translatable text messages with their source locations and unique identifiers, supporting iteration over stored messages. 2. Template Rendering for G",
      "preview_md": "## Task\n**Task Statement: Internationalization Template and Message Catalog System**\n\nImplement a system for extracting, organizing, and rendering translatable messages from documentation sources. The core functionality should:",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_build_gettext.2721e644.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_build_html.d253ea54.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_build_html.d253ea54.lv1",
      "title": "test_build_html.d253ea54.lv1",
      "preview": "Implement a Sphinx documentation system with core functionalities for building, processing, and managing documentation projects. The system should provide: 1. Core functionalities: CSS file registration, pattern matching, document creation, inventory loading, and environment data merging across multiple collector components (dependencies, assets, metadata, titles, toctrees) 2. Main features: Application extensibility",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement a Sphinx documentation system with core functionalities for building, processing, and managing documentation projects. The system should provide:",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_build_html.d253ea54.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_build_linkcheck.20428c8f.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_build_linkcheck.20428c8f.lv1",
      "title": "test_build_linkcheck.20428c8f.lv1",
      "preview": "Hyperlink Validation System Implementation Develop a system to validate external hyperlinks in documentation by implementing: 1. Core Functionalities: Parse HTML content to locate and extract anchor elements for link validation Implement rate limiting mechanisms to control HTTP request frequency and handle server-imposed delays 2. Main Features & Requirements: HTML parsing capability to identify specific anchors with",
      "preview_md": "## Task\n**Task Statement: Hyperlink Validation System Implementation**\n\nDevelop a system to validate external hyperlinks in documentation by implementing:\n\n1. **Core Functionalities:**\n   - Parse HTML content to locate and extract anchor elements for link validation",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_build_linkcheck.20428c8f.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_command_line.039d53a2.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_command_line.039d53a2.lv1",
      "title": "test_command_line.039d53a2.lv1",
      "preview": "Sphinx Documentation Build System Command-Line Interface Core Functionalities: Provide command-line interfaces for building documentation from source files using the Sphinx documentation generator Support multiple output formats (HTML, PDF, LaTeX, text, etc.) through various builders Handle both direct build commands and make-style build workflows Main Features & Requirements: Parse command-line arguments for source/",
      "preview_md": "## Task\n**Task Statement: Sphinx Documentation Build System Command-Line Interface**\n\n**Core Functionalities:**\n- Provide command-line interfaces for building documentation from source files using the Sphinx documentation generator\n- Support multiple output formats (HTML, PDF, LaTeX, text, etc.) through various builders",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_command_line.039d53a2.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_domain_c.4068b9e8.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_domain_c.4068b9e8.lv1",
      "title": "test_domain_c.4068b9e8.lv1",
      "preview": "Implement code generation functionality for C++ domain AST processing, symbol management, intersphinx cross-referencing, and text building. The core objectives are: 1. AST Template Parameter Processing: Implement isPack property detection for C++ template introduction parameters to identify parameter pack expansions in template declarations. 2. Symbol Tree Management: Implement dump method for hierarchical symbol tre",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement code generation functionality for C++ domain AST processing, symbol management, intersphinx cross-referencing, and text building. The core objectives are:",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_domain_c.4068b9e8.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_ext_doctest.530dec5f.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_ext_doctest.530dec5f.lv1",
      "title": "test_ext_doctest.530dec5f.lv1",
      "preview": "Version Compatibility Validation for Documentation Testing Implement a version specification validator that determines whether a given software version satisfies PEP-440 compliant version specifiers. The core functionality should: 1. Parse and validate version specifiers using standard packaging formats (e.g., \">=3.5\", \"<4.0\", \">=2.7,!=3.0\") 2. Compare actual versions against specifications to determine compatibility",
      "preview_md": "## Task\n**Task Statement: Version Compatibility Validation for Documentation Testing**\n\nImplement a version specification validator that determines whether a given software version satisfies PEP-440 compliant version specifiers. The core functionality should:",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_ext_doctest.530dec5f.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_ext_intersphinx.04901873.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_ext_intersphinx.04901873.lv1",
      "title": "test_ext_intersphinx.04901873.lv1",
      "preview": "Intersphinx Cross-Reference System Implementation Implement a cross-reference resolution system that enables linking between different Sphinx documentation projects by: 1. Core Functionalities: Load and parse inventory files from local/remote sources containing project documentation metadata Resolve cross-references to external documentation projects Provide CLI tools for inventory inspection and debugging 2. Main Fe",
      "preview_md": "## Task\n**Task Statement: Intersphinx Cross-Reference System Implementation**\n\nImplement a cross-reference resolution system that enables linking between different Sphinx documentation projects by:\n\n1. **Core Functionalities:**\n   - Load and parse inventory files from local/remote sources containing project documentation metadata",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_ext_intersphinx.04901873.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_ext_math.96576214.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_ext_math.96576214.lv1",
      "title": "test_ext_math.96576214.lv1",
      "preview": "Sphinx Documentation Builder Interface Implementation Core Functionalities: Implement interfaces for a documentation generation system that processes reStructuredText files and converts them into various output formats (HTML, LaTeX, etc.) with advanced features like cross-referencing, syntax highlighting, and mathematical expressions. Main Features & Requirements: Configure HTML asset management policies for optimize",
      "preview_md": "## Task\n**Task Statement: Sphinx Documentation Builder Interface Implementation**\n\n**Core Functionalities:**",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_ext_math.96576214.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_ext_napoleon_docstring.50dabe1f.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_ext_napoleon_docstring.50dabe1f.lv1",
      "title": "test_ext_napoleon_docstring.50dabe1f.lv1",
      "preview": "Task: Implement Sphinx Documentation Processing System Core Functionalities: Load and manage intersphinx inventory mappings for cross-project documentation references Parse and convert Google/NumPy style docstrings to reStructuredText format Process and tokenize type specifications with proper escaping and formatting Main Features & Requirements: Fetch and cache documentation inventories from local/remote sources wit",
      "preview_md": "## Task\n**Task: Implement Sphinx Documentation Processing System**\n\n**Core Functionalities:**\n- Load and manage intersphinx inventory mappings for cross-project documentation references\n- Parse and convert Google/NumPy style docstrings to reStructuredText format\n- Process and tokenize type specifications with proper escaping and formatting",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_ext_napoleon_docstring.50dabe1f.lv1/problem_statement.md"
    },
    {
      "id": "full:sphinx-doc__sphinx.e347e59c/test_util_nodes.08cdc62d.lv1",
      "split": "full",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_util_nodes.08cdc62d.lv1",
      "title": "test_util_nodes.08cdc62d.lv1",
      "preview": "Document Node Processing and Utility Functions Develop utility functions for processing and manipulating document tree nodes in a documentation system. The core functionalities include: 1. Message Extraction: Extract translatable text content from document nodes while handling various node types (literal blocks, images, meta tags) and cleaning formatting artifacts 2. ID Generation: Create unique, URL-safe identifiers",
      "preview_md": "## Task\n**Task Statement: Document Node Processing and Utility Functions**\n\nDevelop utility functions for processing and manipulating document tree nodes in a documentation system. The core functionalities include:",
      "statement_path": "./data/full/sphinx-doc__sphinx.e347e59c/test_util_nodes.08cdc62d.lv1/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_inverse.c240ffe7.lv1",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_inverse.c240ffe7.lv1",
      "title": "test_inverse.c240ffe7.lv1",
      "preview": "Numerical Comparison and Matrix Operations with Domain-Specific Arithmetic Implement functionality for performing approximate numerical comparisons and matrix arithmetic operations over mathematical domains. The core objectives are: 1. Numerical Tolerance Comparison: Develop methods to compare mathematical expressions and numbers with configurable relative/absolute tolerance, handling different precisions and data ty",
      "preview_md": "## Task\n**Task Statement: Numerical Comparison and Matrix Operations with Domain-Specific Arithmetic**\n\nImplement functionality for performing approximate numerical comparisons and matrix arithmetic operations over mathematical domains. The core objectives are:",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_inverse.c240ffe7.lv1/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_nullspace.f14fc970.lv1",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_nullspace.f14fc970.lv1",
      "title": "test_nullspace.f14fc970.lv1",
      "preview": "Implement matrix representation conversion and manipulation interfaces for a domain-based matrix system. The core functionality should provide: 1. Format Conversion: Enable seamless conversion between dense and sparse matrix representations while preserving mathematical properties and domain constraints. 2. Element Access: Implement efficient indexing and slicing operations that handle both scalar element retrieval a",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement matrix representation conversion and manipulation interfaces for a domain-based matrix system. The core functionality should provide:\n\n1. **Format Conversion**: Enable seamless conversion between dense and sparse matrix representations while preserving mathematical properties and domain constraints.",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_nullspace.f14fc970.lv1/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_puiseux.cd575f09.lv1",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_puiseux.cd575f09.lv1",
      "title": "test_puiseux.cd575f09.lv1",
      "preview": "Implement Rational Number and Puiseux Polynomial Arithmetic Systems Core Functionalities: Develop a pure Python rational number class (PythonMPQ) as a fallback for gmpy2's mpq type Create a Puiseux polynomial ring system supporting fractional and negative exponents for truncated series representation Main Features & Requirements: PythonMPQ: Implement complete arithmetic operations (+, -, *, /, **), comparison operato",
      "preview_md": "## Task\n**Task Statement: Implement Rational Number and Puiseux Polynomial Arithmetic Systems**\n\n**Core Functionalities:**\n- Develop a pure Python rational number class (PythonMPQ) as a fallback for gmpy2's mpq type\n- Create a Puiseux polynomial ring system supporting fractional and negative exponents for truncated series representation",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_puiseux.cd575f09.lv1/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_puiseux.cd575f09.lv2",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_puiseux.cd575f09.lv2",
      "title": "test_puiseux.cd575f09.lv2",
      "preview": "Implement Rational Number and Puiseux Polynomial Arithmetic Systems Core Functionalities: Develop a pure Python rational number class (PythonMPQ) as a fallback for gmpy2's mpq type Create a Puiseux polynomial ring system supporting fractional and negative exponents for truncated series representation Main Features & Requirements: PythonMPQ: Implement complete arithmetic operations (+, -, *, /, **), comparison operato",
      "preview_md": "## Task\n**Task Statement: Implement Rational Number and Puiseux Polynomial Arithmetic Systems**\n\n**Core Functionalities:**\n- Develop a pure Python rational number class (PythonMPQ) as a fallback for gmpy2's mpq type\n- Create a Puiseux polynomial ring system supporting fractional and negative exponents for truncated series representation",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_puiseux.cd575f09.lv2/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_runtests_pytest.71d43c5f.lv1",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_runtests_pytest.71d43c5f.lv1",
      "title": "test_runtests_pytest.71d43c5f.lv1",
      "preview": "Path Resolution and Test Configuration for SymPy Testing Framework Develop utilities to handle path conversion and argument processing for a pytest-based testing system that maintains backward compatibility with SymPy's legacy test runner. Core Functionalities: Convert partial/relative paths to absolute paths within the SymPy directory structure Process and validate test paths, keywords, and configuration arguments f",
      "preview_md": "## Task\n**Task Statement: Path Resolution and Test Configuration for SymPy Testing Framework**\n\nDevelop utilities to handle path conversion and argument processing for a pytest-based testing system that maintains backward compatibility with SymPy's legacy test runner.\n\n**Core Functionalities:**",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_runtests_pytest.71d43c5f.lv1/problem_statement.md"
    },
    {
      "id": "full:sympy__sympy.c1097516/test_smtlib.69d52b90.lv1",
      "split": "full",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_smtlib.69d52b90.lv1",
      "title": "test_smtlib.69d52b90.lv1",
      "preview": "Implement numeric value checking and conversion utilities for symbolic mathematics, focusing on: 1. Core Functionalities: Check if numeric values represent integers (regardless of their internal type representation) Convert decimal numbers to rational form while preserving precision information Generate SMT-LIB compatible code from symbolic mathematical expressions 2. Main Features and Requirements: Handle various nu",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement numeric value checking and conversion utilities for symbolic mathematics, focusing on:\n\n1. **Core Functionalities:**\n   - Check if numeric values represent integers (regardless of their internal type representation)\n   - Convert decimal numbers to rational form while preserving precision information",
      "statement_path": "./data/full/sympy__sympy.c1097516/test_smtlib.69d52b90.lv1/problem_statement.md"
    },
    {
      "id": "lite:astropy__astropy.b0db0daa/test_quantity_erfa_ufuncs.3fc54395.lv1",
      "split": "lite",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_quantity_erfa_ufuncs.3fc54395.lv1",
      "title": "test_quantity_erfa_ufuncs.3fc54395.lv1",
      "preview": "Implement Structured Unit Operations and Quantity Function Helpers Core Functionalities: Implement arithmetic and indexing operations for structured units (units with multiple named fields like position/velocity pairs) Provide unit-aware helper functions for NumPy array operations that preserve dimensional analysis Support mathematical operations (einsum, norm, concatenation) while maintaining proper unit propagation",
      "preview_md": "## Task\n**Task Statement: Implement Structured Unit Operations and Quantity Function Helpers**\n\n**Core Functionalities:**\n- Implement arithmetic and indexing operations for structured units (units with multiple named fields like position/velocity pairs)\n- Provide unit-aware helper functions for NumPy array operations that preserve dimensional analysis",
      "statement_path": "./data/lite/astropy__astropy.b0db0daa/test_quantity_erfa_ufuncs.3fc54395.lv1/problem_statement.md"
    },
    {
      "id": "lite:astropy__astropy.b0db0daa/test_table.48eef659.lv1",
      "split": "lite",
      "repo_ref": "astropy__astropy.b0db0daa",
      "repo": "astropy/astropy",
      "case": "test_table.48eef659.lv1",
      "title": "test_table.48eef659.lv1",
      "preview": "Implement VOTable Data Processing and Table Operations Core Functionalities: Process and manipulate astronomical data tables in VOTable format with support for various data types (numeric, string, complex, boolean, arrays) Handle table creation, modification, and serialization operations including column addition, row manipulation, and format conversion Provide data validation and format compliance checking for VOTab",
      "preview_md": "## Task\n**Task Statement: Implement VOTable Data Processing and Table Operations**\n\n**Core Functionalities:**\n- Process and manipulate astronomical data tables in VOTable format with support for various data types (numeric, string, complex, boolean, arrays)",
      "statement_path": "./data/lite/astropy__astropy.b0db0daa/test_table.48eef659.lv1/problem_statement.md"
    },
    {
      "id": "lite:huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv2",
      "split": "lite",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_modeling_seggpt.f9e83f60.lv2",
      "title": "test_modeling_seggpt.f9e83f60.lv2",
      "preview": "Model Interface Implementation and Configuration Implement code generation for transformer model interfaces focusing on: 1. Core Functionalities: Input embedding retrieval from neural network modules Model configuration initialization with parameter validation and setup 2. Main Features & Requirements: Handle various embedding layer architectures (standard, encoder-decoder, vision-language models) Support flexible co",
      "preview_md": "## Task\n**Task Statement: Model Interface Implementation and Configuration**\n\nImplement code generation for transformer model interfaces focusing on:\n\n1. **Core Functionalities**:\n   - Input embedding retrieval from neural network modules\n   - Model configuration initialization with parameter validation and setup",
      "statement_path": "./data/lite/huggingface__transformers.e2e8dbed/test_modeling_seggpt.f9e83f60.lv2/problem_statement.md"
    },
    {
      "id": "lite:huggingface__transformers.e2e8dbed/test_processing_wav2vec2.4f660c78.lv1",
      "split": "lite",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_processing_wav2vec2.4f660c78.lv1",
      "title": "test_processing_wav2vec2.4f660c78.lv1",
      "preview": "Automatic Tokenizer Management and Speech-to-Text Tokenization Core Functionalities: Automatically instantiate appropriate tokenizer classes from pretrained models based on model configuration Provide specialized tokenization for speech-to-text models (Wav2Vec2) with CTC decoding capabilities Handle dynamic tokenizer class resolution and vocabulary management Main Features & Requirements: Auto-detection of tokenizer ",
      "preview_md": "## Task\n**Task Statement: Automatic Tokenizer Management and Speech-to-Text Tokenization**\n\n**Core Functionalities:**\n- Automatically instantiate appropriate tokenizer classes from pretrained models based on model configuration\n- Provide specialized tokenization for speech-to-text models (Wav2Vec2) with CTC decoding capabilities",
      "statement_path": "./data/lite/huggingface__transformers.e2e8dbed/test_processing_wav2vec2.4f660c78.lv1/problem_statement.md"
    },
    {
      "id": "lite:huggingface__transformers.e2e8dbed/test_serve.4e7860c7.lv1",
      "split": "lite",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_serve.4e7860c7.lv1",
      "title": "test_serve.4e7860c7.lv1",
      "preview": "OpenAI-Compatible Model Serving Interface Implement a streaming chat completion service that provides OpenAI-compatible API endpoints for serving transformer models. The core functionalities include: 1. Model Management: Dynamic loading/unloading of models with timeout-based memory management 2. Chat Completion Streaming: Generate real-time streaming responses for chat conversations with support for tool calls and mu",
      "preview_md": "## Task\n**Task Statement: OpenAI-Compatible Model Serving Interface**\n\nImplement a streaming chat completion service that provides OpenAI-compatible API endpoints for serving transformer models. The core functionalities include:\n\n1. **Model Management**: Dynamic loading/unloading of models with timeout-based memory management",
      "statement_path": "./data/lite/huggingface__transformers.e2e8dbed/test_serve.4e7860c7.lv1/problem_statement.md"
    },
    {
      "id": "lite:huggingface__transformers.e2e8dbed/test_tests_fetcher.e1abe0dd.lv1",
      "split": "lite",
      "repo_ref": "huggingface__transformers.e2e8dbed",
      "repo": "huggingface/transformers",
      "case": "test_tests_fetcher.e1abe0dd.lv1",
      "title": "test_tests_fetcher.e1abe0dd.lv1",
      "preview": "Intelligent Test Selection and Dependency Analysis System Core Functionalities: Analyze code changes in a Git repository to identify modified files and their dependencies Build reverse dependency maps to determine which tests are impacted by specific code changes Intelligently filter and select only relevant tests to run based on file modifications Main Features & Requirements: Parse Git diffs and commit messages to ",
      "preview_md": "## Task\n**Task Statement: Intelligent Test Selection and Dependency Analysis System**\n\n**Core Functionalities:**\n- Analyze code changes in a Git repository to identify modified files and their dependencies\n- Build reverse dependency maps to determine which tests are impacted by specific code changes",
      "statement_path": "./data/lite/huggingface__transformers.e2e8dbed/test_tests_fetcher.e1abe0dd.lv1/problem_statement.md"
    },
    {
      "id": "lite:huggingface__trl.02a34777/test_data_utils.827a9d15.lv1",
      "split": "lite",
      "repo_ref": "huggingface__trl.02a34777",
      "repo": "huggingface/trl",
      "case": "test_data_utils.827a9d15.lv1",
      "title": "test_data_utils.827a9d15.lv1",
      "preview": "Conversational Dataset Processing and Multimodal Message Handling Core Functionalities: Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal) Apply chat templates to convert structured conversations into tokenizer-ready text Pack and optimize dataset sequences for efficient training Main Features & Requirements: Handle multimodal messages with text and image co",
      "preview_md": "## Task\n**Task Statement: Conversational Dataset Processing and Multimodal Message Handling**\n\n**Core Functionalities:**\n- Process and transform conversational datasets between different formats (ChatML, preference pairs, multimodal)\n- Apply chat templates to convert structured conversations into tokenizer-ready text",
      "statement_path": "./data/lite/huggingface__trl.02a34777/test_data_utils.827a9d15.lv1/problem_statement.md"
    },
    {
      "id": "lite:Lightning-AI__pytorch-lightning.126fa6f1/test_connector.825ef3b1.lv1",
      "split": "lite",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_connector.825ef3b1.lv1",
      "title": "test_connector.825ef3b1.lv1",
      "preview": "Implement Lightning Fabric Component Configuration and Quantized Linear Layer Integration Core Functionalities: Configure and initialize Lightning Fabric components (accelerators, strategies, precision plugins, cluster environments) Implement bitsandbytes quantized linear layers with 8-bit and 4-bit precision modes Provide seamless integration between hardware acceleration and model quantization Main Features & Requi",
      "preview_md": "## Task\n**Task Statement: Implement Lightning Fabric Component Configuration and Quantized Linear Layer Integration**\n\n**Core Functionalities:**\n- Configure and initialize Lightning Fabric components (accelerators, strategies, precision plugins, cluster environments)\n- Implement bitsandbytes quantized linear layers with 8-bit and 4-bit precision modes",
      "statement_path": "./data/lite/Lightning-AI__pytorch-lightning.126fa6f1/test_connector.825ef3b1.lv1/problem_statement.md"
    },
    {
      "id": "lite:Lightning-AI__pytorch-lightning.126fa6f1/test_data.c8b292af.lv1",
      "split": "lite",
      "repo_ref": "Lightning-AI__pytorch-lightning.126fa6f1",
      "repo": "Lightning-AI/pytorch-lightning",
      "case": "test_data.c8b292af.lv1",
      "title": "test_data.c8b292af.lv1",
      "preview": "DataLoader Management and Optimization Utilities Develop a comprehensive data loading utility system that provides: 1. Core Functionalities: DataLoader introspection and validation (dataset type detection, length checking) Dynamic DataLoader reconstruction with custom samplers and configurations Automatic performance optimization (worker count suggestions, epoch-aware sampling) 2. Main Features & Requirements: Suppor",
      "preview_md": "## Task\n**Task Statement: DataLoader Management and Optimization Utilities**\n\nDevelop a comprehensive data loading utility system that provides:\n\n1. **Core Functionalities:**\n   - DataLoader introspection and validation (dataset type detection, length checking)\n   - Dynamic DataLoader reconstruction with custom samplers and configurations",
      "statement_path": "./data/lite/Lightning-AI__pytorch-lightning.126fa6f1/test_data.c8b292af.lv1/problem_statement.md"
    },
    {
      "id": "lite:linkedin__Liger-Kernel.c856fbab/test_fused_neighborhood_attention.78217be4.lv2",
      "split": "lite",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_fused_neighborhood_attention.78217be4.lv2",
      "title": "test_fused_neighborhood_attention.78217be4.lv2",
      "preview": "Implement Fused Neighborhood Attention Mechanism Develop an efficient neighborhood attention system that restricts attention computation to local windows around each position, reducing complexity from O(n) to O(n*k). The implementation should provide: Core Functionalities: Localized attention computation within configurable neighborhood windows Fused operations combining query-key-value projections with attention ca",
      "preview_md": "## Task\n**Task Statement: Implement Fused Neighborhood Attention Mechanism**\n\nDevelop an efficient neighborhood attention system that restricts attention computation to local windows around each position, reducing complexity from O(n) to O(n*k). The implementation should provide:\n\n**Core Functionalities:**",
      "statement_path": "./data/lite/linkedin__Liger-Kernel.c856fbab/test_fused_neighborhood_attention.78217be4.lv2/problem_statement.md"
    },
    {
      "id": "lite:linkedin__Liger-Kernel.c856fbab/test_poly_norm.7b0e3399.lv1",
      "split": "lite",
      "repo_ref": "linkedin__Liger-Kernel.c856fbab",
      "repo": "linkedin/Liger-Kernel",
      "case": "test_poly_norm.7b0e3399.lv1",
      "title": "test_poly_norm.7b0e3399.lv1",
      "preview": "Implement Polynomial Normalization Function Develop a polynomial normalization operation that applies advanced normalization techniques to input tensors using learnable weight and bias parameters. The function should support configurable epsilon values for numerical stability and optional in-place computation for memory efficiency. Key considerations include maintaining numerical precision, supporting gradient comput",
      "preview_md": "## Task\n**Task Statement: Implement Polynomial Normalization Function**",
      "statement_path": "./data/lite/linkedin__Liger-Kernel.c856fbab/test_poly_norm.7b0e3399.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_databricks_tracing_utils.8ef44eb4.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_databricks_tracing_utils.8ef44eb4.lv1",
      "title": "test_databricks_tracing_utils.8ef44eb4.lv1",
      "preview": "MLflow Tracing and Assessment Data Management Core Functionalities: Implement data serialization/deserialization between MLflow entities and protocol buffer formats for distributed tracing systems Manage span lifecycle operations including input/output handling and attribute management Handle assessment source type validation and standardization with backward compatibility Main Features & Requirements: Convert trace ",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing and Assessment Data Management**\n\n**Core Functionalities:**\n- Implement data serialization/deserialization between MLflow entities and protocol buffer formats for distributed tracing systems\n- Manage span lifecycle operations including input/output handling and attribute management",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_databricks_tracing_utils.8ef44eb4.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_judge_tool_search_traces.cad8fc4b.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_judge_tool_search_traces.cad8fc4b.lv1",
      "title": "test_judge_tool_search_traces.cad8fc4b.lv1",
      "preview": "Assessment Source Management and Trace Search Tool Implementation Core Functionalities: Standardize and validate assessment source types (human, LLM judge, code-based) with proper deprecation handling Provide trace search capabilities within MLflow experiments with filtering, ordering, and result pagination Convert assessment data between internal formats and tool-compatible types Main Features & Requirements: Suppor",
      "preview_md": "## Task\n**Task Statement: Assessment Source Management and Trace Search Tool Implementation**\n\n**Core Functionalities:**\n- Standardize and validate assessment source types (human, LLM judge, code-based) with proper deprecation handling\n- Provide trace search capabilities within MLflow experiments with filtering, ordering, and result pagination",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_judge_tool_search_traces.cad8fc4b.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_responses_agent.072c4133.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_responses_agent.072c4133.lv1",
      "title": "test_responses_agent.072c4133.lv1",
      "preview": "MLflow Model Interface Implementation Implement a comprehensive model management system that provides: 1. Core Functionalities: Model lifecycle management (creation, loading, logging, validation) Multi-format model support with flavor-specific configurations Distributed tracing and span management for model operations Type-safe data validation and schema inference Artifact and dependency management 2. Main Features &",
      "preview_md": "## Task\n**Task Statement: MLflow Model Interface Implementation**\n\nImplement a comprehensive model management system that provides:\n\n1. **Core Functionalities:**\n   - Model lifecycle management (creation, loading, logging, validation)\n   - Multi-format model support with flavor-specific configurations",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_responses_agent.072c4133.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_serialization.2c029be6.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_serialization.2c029be6.lv1",
      "title": "test_serialization.2c029be6.lv1",
      "preview": "Assessment Source Management and Custom Scorer Framework Core Functionalities: Implement assessment source type validation and standardization for tracking evaluation origins (human, LLM judge, code-based) Create a flexible scorer framework supporting custom evaluation functions with serialization/deserialization capabilities Main Features & Requirements: Parse, validate, and standardize assessment source types with ",
      "preview_md": "## Task\n**Task Statement: Assessment Source Management and Custom Scorer Framework**\n\n**Core Functionalities:**\n- Implement assessment source type validation and standardization for tracking evaluation origins (human, LLM judge, code-based)",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_serialization.2c029be6.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_span.69efd376.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_span.69efd376.lv1",
      "title": "test_span.69efd376.lv1",
      "preview": "Implement MLflow Distributed Tracing System Core Functionalities: Create and manage distributed tracing spans with hierarchical relationships Support both live (mutable) and immutable span states during execution lifecycle Provide OpenTelemetry protocol compatibility for trace data exchange Enable trace data serialization/deserialization across different formats (dict, protobuf) Main Features & Requirements: Span lif",
      "preview_md": "## Task\n**Task Statement: Implement MLflow Distributed Tracing System**\n\n**Core Functionalities:**\n- Create and manage distributed tracing spans with hierarchical relationships\n- Support both live (mutable) and immutable span states during execution lifecycle\n- Provide OpenTelemetry protocol compatibility for trace data exchange",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_span.69efd376.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_trace.17fde8b0.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_trace.17fde8b0.lv1",
      "title": "test_trace.17fde8b0.lv1",
      "preview": "MLflow Tracing System Implementation Core Functionalities: Implement a comprehensive distributed tracing system for MLflow that captures, manages, and analyzes execution flows across ML workflows and applications. Main Features & Requirements: Span Management: Create and manage hierarchical execution spans with lifecycle control, attributes, inputs/outputs, events, and status tracking Trace Data Handling: Store and r",
      "preview_md": "## Task\n**Task Statement: MLflow Tracing System Implementation**\n\n**Core Functionalities:**\nImplement a comprehensive distributed tracing system for MLflow that captures, manages, and analyzes execution flows across ML workflows and applications.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_trace.17fde8b0.lv1/problem_statement.md"
    },
    {
      "id": "lite:mlflow__mlflow.93dab383/test_validation.2e1c5076.lv1",
      "split": "lite",
      "repo_ref": "mlflow__mlflow.93dab383",
      "repo": "mlflow/mlflow",
      "case": "test_validation.2e1c5076.lv1",
      "title": "test_validation.2e1c5076.lv1",
      "preview": "MLflow Data Validation System Implement a comprehensive validation system for MLflow experiment tracking that ensures data integrity and enforces business rules across machine learning workflows. Core Functionalities: Validate naming conventions and formats for metrics, parameters, tags, and model aliases Enforce length limits and character restrictions on various MLflow entities Check batch logging constraints and d",
      "preview_md": "## Task\n**Task Statement: MLflow Data Validation System**\n\nImplement a comprehensive validation system for MLflow experiment tracking that ensures data integrity and enforces business rules across machine learning workflows.\n\n**Core Functionalities:**\n- Validate naming conventions and formats for metrics, parameters, tags, and model aliases",
      "statement_path": "./data/lite/mlflow__mlflow.93dab383/test_validation.2e1c5076.lv1/problem_statement.md"
    },
    {
      "id": "lite:mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv1",
      "split": "lite",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_algorithms.1f0181c2.lv1",
      "title": "test_algorithms.1f0181c2.lv1",
      "preview": "Statistical Bootstrap Resampling Implementation Implement a bootstrap resampling algorithm that: 1. Core Functionality: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics 2. Key Features: Supports multiple resampling iterations with configurable sample sizes Handles various summary functions (mean, median, etc.) with automatic NaN-aware alter",
      "preview_md": "## Task\n**Task Statement: Statistical Bootstrap Resampling Implementation**\n\nImplement a bootstrap resampling algorithm that:\n\n1. **Core Functionality**: Performs statistical resampling with replacement on one or more input arrays to generate distributions of aggregate statistics\n\n2. **Key Features**:",
      "statement_path": "./data/lite/mwaskom__seaborn.7001ebe7/test_algorithms.1f0181c2.lv1/problem_statement.md"
    },
    {
      "id": "lite:mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv1",
      "split": "lite",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_regression.ce8c62e2.lv1",
      "title": "test_regression.ce8c62e2.lv1",
      "preview": "Statistical Regression Plotting and Visualization Core Functionalities: Create scatter plots with fitted regression lines and confidence intervals Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed) Handle data preprocessing including variable extraction, missing data removal, and binning Generate both single plots and multi-faceted grid visualizations Main Features & Re",
      "preview_md": "## Task\n**Task Statement: Statistical Regression Plotting and Visualization**\n\n**Core Functionalities:**\n- Create scatter plots with fitted regression lines and confidence intervals\n- Support multiple regression models (linear, polynomial, logistic, robust, lowess, log-transformed)",
      "statement_path": "./data/lite/mwaskom__seaborn.7001ebe7/test_regression.ce8c62e2.lv1/problem_statement.md"
    },
    {
      "id": "lite:mwaskom__seaborn.7001ebe7/test_statistics.0f2ae277.lv1",
      "split": "lite",
      "repo_ref": "mwaskom__seaborn.7001ebe7",
      "repo": "mwaskom/seaborn",
      "case": "test_statistics.0f2ae277.lv1",
      "title": "test_statistics.0f2ae277.lv1",
      "preview": "Statistical Data Preprocessing for Visualization Implement statistical transformation interfaces that prepare data for visualization by defining computational parameters and support structures. The core functionalities include: 1. KDE Support Definition: Create evaluation grids for kernel density estimation on univariate/bivariate data, handling bandwidth calculation, boundary clipping, and grid spacing. 2. Histogram",
      "preview_md": "## Task\n**Task Statement: Statistical Data Preprocessing for Visualization**\n\nImplement statistical transformation interfaces that prepare data for visualization by defining computational parameters and support structures. The core functionalities include:",
      "statement_path": "./data/lite/mwaskom__seaborn.7001ebe7/test_statistics.0f2ae277.lv1/problem_statement.md"
    },
    {
      "id": "lite:Netflix__metaflow.b390a8d4/test_stub_generator.7bf08c98.lv1",
      "split": "lite",
      "repo_ref": "Netflix__metaflow.b390a8d4",
      "repo": "Netflix/metaflow",
      "case": "test_stub_generator.7bf08c98.lv1",
      "title": "test_stub_generator.7bf08c98.lv1",
      "preview": "Python Type Stub Generation for Dynamic Code Analysis Generate Python type stub (.pyi) files by dynamically analyzing runtime code structures. The system should: 1. Core Functionality: Introspect live Python modules to extract type information and generate corresponding stub files with proper type annotations, signatures, and documentation. 2. Key Features: Parse function signatures, class hierarchies, and type annot",
      "preview_md": "## Task\n**Task Statement: Python Type Stub Generation for Dynamic Code Analysis**\n\nGenerate Python type stub (.pyi) files by dynamically analyzing runtime code structures. The system should:",
      "statement_path": "./data/lite/Netflix__metaflow.b390a8d4/test_stub_generator.7bf08c98.lv1/problem_statement.md"
    },
    {
      "id": "lite:pandas-dev__pandas.82fa2715/test_concat.ebe5de39.lv1",
      "split": "lite",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_concat.ebe5de39.lv1",
      "title": "test_concat.ebe5de39.lv1",
      "preview": "Pandas Data Type Inference and Conversion System Core Functionalities: Implement a comprehensive data type inference, validation, and conversion system for pandas that handles diverse data structures including arrays, scalars, categorical data, sparse arrays, and datetime-like objects. Main Features & Requirements: Type Detection: Identify and classify various data types (dict-like, named tuples, numeric, datetime, c",
      "preview_md": "## Task\n**Task Statement: Pandas Data Type Inference and Conversion System**\n\n**Core Functionalities:**\nImplement a comprehensive data type inference, validation, and conversion system for pandas that handles diverse data structures including arrays, scalars, categorical data, sparse arrays, and datetime-like objects.\n\n**Main Features & Requirements:**",
      "statement_path": "./data/lite/pandas-dev__pandas.82fa2715/test_concat.ebe5de39.lv1/problem_statement.md"
    },
    {
      "id": "lite:pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv2",
      "split": "lite",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_iceberg.85771c70.lv2",
      "title": "test_iceberg.85771c70.lv2",
      "preview": "Apache Iceberg Table Integration Implement functionality to read Apache Iceberg tables into pandas DataFrames with support for: Core Functionalities: Connect to Iceberg catalogs and load table data Convert Iceberg table format to pandas DataFrame format Support flexible data filtering and column selection Main Features & Requirements: Table identification and catalog configuration management Row filtering with custom",
      "preview_md": "## Task\n**Task Statement: Apache Iceberg Table Integration**\n\nImplement functionality to read Apache Iceberg tables into pandas DataFrames with support for:\n\n**Core Functionalities:**\n- Connect to Iceberg catalogs and load table data\n- Convert Iceberg table format to pandas DataFrame format",
      "statement_path": "./data/lite/pandas-dev__pandas.82fa2715/test_iceberg.85771c70.lv2/problem_statement.md"
    },
    {
      "id": "lite:pandas-dev__pandas.82fa2715/test_list_accessor.7ab0b2ea.lv1",
      "split": "lite",
      "repo_ref": "pandas-dev__pandas.82fa2715",
      "repo": "pandas-dev/pandas",
      "case": "test_list_accessor.7ab0b2ea.lv1",
      "title": "test_list_accessor.7ab0b2ea.lv1",
      "preview": "Implement Arrow-backed data accessors and type conversion utilities for pandas that provide: 1. Core functionalities: Specialized accessor classes for Arrow list and struct data types that enable element-wise operations, validation, and data manipulation on PyArrow-backed pandas Series. 2. Main features and requirements: List accessor supporting indexing, slicing, length calculation, flattening, and iteration control",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement Arrow-backed data accessors and type conversion utilities for pandas that provide:\n\n1. **Core functionalities**: Specialized accessor classes for Arrow list and struct data types that enable element-wise operations, validation, and data manipulation on PyArrow-backed pandas Series.",
      "statement_path": "./data/lite/pandas-dev__pandas.82fa2715/test_list_accessor.7ab0b2ea.lv1/problem_statement.md"
    },
    {
      "id": "lite:pydantic__pydantic.e1dcaf9e/test_deprecated_fields.40a2ec54.lv1",
      "split": "lite",
      "repo_ref": "pydantic__pydantic.e1dcaf9e",
      "repo": "pydantic/pydantic",
      "case": "test_deprecated_fields.40a2ec54.lv1",
      "title": "test_deprecated_fields.40a2ec54.lv1",
      "preview": "Implement Property and Field Management System Create a system that handles property descriptors, field metadata, and computed fields for data validation models. The core functionalities include: 1. Property Descriptor Management: Convert functions to properties, handle deprecated field access with runtime warnings, and manage private attribute detection 2. Field Metadata Processing: Collect and merge field informati",
      "preview_md": "## Task\n**Task Statement: Implement Property and Field Management System**\n\nCreate a system that handles property descriptors, field metadata, and computed fields for data validation models. The core functionalities include:",
      "statement_path": "./data/lite/pydantic__pydantic.e1dcaf9e/test_deprecated_fields.40a2ec54.lv1/problem_statement.md"
    },
    {
      "id": "lite:pydata__xarray.97f3a746/test_backends_chunks.fa55f68a.lv1",
      "split": "lite",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_backends_chunks.fa55f68a.lv1",
      "title": "test_backends_chunks.fa55f68a.lv1",
      "preview": "Multi-dimensional Array Chunk Alignment and Grid-based Rechunking Core Functionalities: Align chunked array data structures with backend storage grid patterns Generate uniform grid-based chunk layouts for multi-dimensional arrays Rechunk variables to optimize memory usage and storage alignment Main Features & Requirements: Handle multi-dimensional arrays with varying chunk sizes across dimensions Preserve data integr",
      "preview_md": "## Task\n**Task Statement: Multi-dimensional Array Chunk Alignment and Grid-based Rechunking**\n\n**Core Functionalities:**\n- Align chunked array data structures with backend storage grid patterns\n- Generate uniform grid-based chunk layouts for multi-dimensional arrays\n- Rechunk variables to optimize memory usage and storage alignment",
      "statement_path": "./data/lite/pydata__xarray.97f3a746/test_backends_chunks.fa55f68a.lv1/problem_statement.md"
    },
    {
      "id": "lite:pydata__xarray.97f3a746/test_treenode.aa8ba777.lv2",
      "split": "lite",
      "repo_ref": "pydata__xarray.97f3a746",
      "repo": "pydata/xarray",
      "case": "test_treenode.aa8ba777.lv2",
      "title": "test_treenode.aa8ba777.lv2",
      "preview": "Tree Data Structure Implementation Implement a hierarchical tree node system that supports: 1. Core Functionalities: Parent-child relationships with bidirectional navigation Named and unnamed node variants Tree traversal operations (ancestors, descendants, siblings) Path-based node access using Unix-like syntax 2. Main Features: Node lifecycle management (attach, detach, orphan operations) Tree structure validation a",
      "preview_md": "## Task\n**Task Statement: Tree Data Structure Implementation**\n\nImplement a hierarchical tree node system that supports:\n\n1. **Core Functionalities:**\n   - Parent-child relationships with bidirectional navigation\n   - Named and unnamed node variants",
      "statement_path": "./data/lite/pydata__xarray.97f3a746/test_treenode.aa8ba777.lv2/problem_statement.md"
    },
    {
      "id": "lite:sphinx-doc__sphinx.e347e59c/test_domain_c.4068b9e8.lv1",
      "split": "lite",
      "repo_ref": "sphinx-doc__sphinx.e347e59c",
      "repo": "sphinx-doc/sphinx",
      "case": "test_domain_c.4068b9e8.lv1",
      "title": "test_domain_c.4068b9e8.lv1",
      "preview": "Implement code generation functionality for C++ domain AST processing, symbol management, intersphinx cross-referencing, and text building. The core objectives are: 1. AST Template Parameter Processing: Implement isPack property detection for C++ template introduction parameters to identify parameter pack expansions in template declarations. 2. Symbol Tree Management: Implement dump method for hierarchical symbol tre",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement code generation functionality for C++ domain AST processing, symbol management, intersphinx cross-referencing, and text building. The core objectives are:",
      "statement_path": "./data/lite/sphinx-doc__sphinx.e347e59c/test_domain_c.4068b9e8.lv1/problem_statement.md"
    },
    {
      "id": "lite:sympy__sympy.c1097516/test_nullspace.f14fc970.lv1",
      "split": "lite",
      "repo_ref": "sympy__sympy.c1097516",
      "repo": "sympy/sympy",
      "case": "test_nullspace.f14fc970.lv1",
      "title": "test_nullspace.f14fc970.lv1",
      "preview": "Implement matrix representation conversion and manipulation interfaces for a domain-based matrix system. The core functionality should provide: 1. Format Conversion: Enable seamless conversion between dense and sparse matrix representations while preserving mathematical properties and domain constraints. 2. Element Access: Implement efficient indexing and slicing operations that handle both scalar element retrieval a",
      "preview_md": "## Task\n**Task Statement:**\n\nImplement matrix representation conversion and manipulation interfaces for a domain-based matrix system. The core functionality should provide:\n\n1. **Format Conversion**: Enable seamless conversion between dense and sparse matrix representations while preserving mathematical properties and domain constraints.",
      "statement_path": "./data/lite/sympy__sympy.c1097516/test_nullspace.f14fc970.lv1/problem_statement.md"
    }
  ]
}
