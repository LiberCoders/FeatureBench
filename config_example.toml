# ==============================================================================
# Global Environment Variables (injected into each container at runtime)
# ==============================================================================
[env_vars]
HF_ENDPOINT=""
HF_TOKEN=""
GITHUB_TOKEN=""

# ==============================================================================
# DataPipeline Configuration
# ==============================================================================

# LLM Basic Configuration
[llm_config]
llm_name="" # e.g., "azure-gpt-5-fy0828" / "local"
llm_temperature=0.0
llm_max_tokens=65536    # Maximum output tokens (-1 = no limit)
timeout=180

[llm.azure-gpt-5-fy0828]
model = "azure/gpt-5-fy0828"
api_key = ""
base_url = ""

[llm.claude-sonnet-4-20250514]
model = "claude-sonnet-4-20250514"
api_key = ""
base_url = [
    "",
    "",
]

[llm.local]
backend = "vllm"                                     # Backend type: vllm
base_url = "http://localhost:8080/v1"                # vLLM server URL

# ==============================================================================
# Inference Agent Configuration
# ==============================================================================

# Inference runtime settings
[infer]
# Optional: Host cache directory for agent downloads; will be mounted to /download inside containers
# Set to empty to disable cache mount (each container will re-download/install)
# Use an absolute path here
download_cache_dir="/path/to/ACE-Bench/download_cache"

# OpenHands Agent Configuration
[infer_config.openhands]
# Required: LLM API Key (supports OpenAI/Anthropic/Azure, etc.)
LLM_API_KEY = ""
# Optional: LLM model name (defaults to --model argument from command line)
LLM_MODEL = ""
# Optional: Reasoning effort for OpenAI o-series models
LLM_REASONING_EFFORT = ""
# Optional: LLM API Base URL
LLM_BASE_URL = ""
# Azure API Version (required for Azure models only)
LLM_API_VERSION = ""
# Optional: OpenHands agent max iterations (step limit). Upstream default is 500.
OPENHANDS_MAX_ITERATIONS = ""
# Optional: Lock OpenHands version (leave empty to use latest)
OPENHANDS_VERSION = "0.62.0"
# Optional: Save llm completions if true
SAVE_COMPLETIONS = false

# Claude Code Agent Configuration
[infer_config.claude_code]
# Required: Anthropic API Key
ANTHROPIC_API_KEY = ""
# Optional: Specify the model to use (defaults to --model argument from command line)
ANTHROPIC_MODEL = ""
# Optional: Custom API Base URL
ANTHROPIC_BASE_URL = ""
# Optional: Lock Claude Code version (leave empty to use latest)
CLAUDE_CODE_VERSION = ""

# Gemini CLI Agent Configuration
[infer_config.gemini_cli]
GEMINI_API_KEY = ""
GEMINI_MODEL = ""
GOOGLE_GEMINI_BASE_URL = ""

# Codex Agent Configuration (Azure OpenAI supported)
[infer_config.codex]
# Required: OpenAI API Key
OPENAI_API_KEY = ""
# Optional: Specify the model to use (defaults to --model argument from command line)
CODEX_MODEL = ""
# Optional: Custom API Base URL
OPENAI_BASE_URL = ""
# Optional: Reasoning effort level for Codex reasoning models; empty defaults to "medium"
CODEX_REASONING_EFFORT = ""
